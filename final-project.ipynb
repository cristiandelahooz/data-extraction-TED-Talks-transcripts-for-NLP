{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "# An√°lisis de Popularidad de TED Talks mediante Procesamiento de Lenguaje Natural\n",
    "\n",
    "## Proyecto Final - Materia de NLP\n",
    "\n",
    "### Equipo de Trabajo\n",
    "- **Manuel Rodriguez** - ID: 1015-0681  \n",
    "- **Carolina Bencosme** - ID: 1014-8929  \n",
    "- **Cristian de la Hoz** - ID: 1014-9779  \n",
    "\n",
    "**Fecha:** Agosto 2025  \n",
    "**Objetivo:** Determinar los factores que hacen popular un TED Talk utilizando t√©cnicas avanzadas de extracci√≥n de informaci√≥n y machine learning\n",
    "\n",
    "---\n",
    "\n",
    "### üî¥ IMPORTANTE - Repositorio GitHub\n",
    "\n",
    "<div style=\"background-color: #ffebee; border: 3px solid #e53e3e; padding: 15px; margin: 10px 0; border-radius: 8px;\">\n",
    "<h4 style=\"color: #c53030; margin-top: 0;\">üìç UBICACI√ìN DEL PROYECTO COMPLETO</h4>\n",
    "\n",
    "**El proyecto se encuentra en GitHub, no fue subido junto con el resto de la asignaci√≥n debido al l√≠mite en la PVA. El proyecto se vio en la necesidad de ser modularizado debido a que era bastante extenso, incluso luego de hacerlo el mismo sigue siendo muy extenso.**\n",
    "\n",
    "**Para visualizar las implementaciones completas, favor de dirigirse al repositorio:**\n",
    "\n",
    "üîó **https://github.com/cristiandelahooz/data-extraction-TED-Talks-transcripts-for-NLP**\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### Resumen Ejecutivo\n",
    "\n",
    "Este proyecto analiza el dataset `ted_talks_en.csv` para identificar patrones que determinan la popularidad de las charlas TED. Mediante t√©cnicas de NLP como Named Entity Recognition (NER), an√°lisis de sentimientos y extracci√≥n de caracter√≠sticas textuales, se entrenan m√∫ltiples modelos de machine learning para clasificar videos en categor√≠as de popularidad.\n",
    "\n",
    "**Meta Principal:** Alcanzar un F1-score superior a 0.78 en la clasificaci√≥n de popularidad.\n",
    "\n",
    "### Estructura del Proyecto\n",
    "\n",
    "``` bash\n",
    "data-extraction-TED-Talks-transcripts-for-NLP/\n",
    "‚îú‚îÄ‚îÄ final-project.ipynb          # Notebook principal documentado\n",
    "‚îú‚îÄ‚îÄ ted_talks_en.csv            # Dataset de TED Talks\n",
    "‚îú‚îÄ‚îÄ model_summary.txt           # Resumen de resultados de modelos\n",
    "‚îî‚îÄ‚îÄ modules/                    # M√≥dulos especializados\n",
    "    ‚îú‚îÄ‚îÄ __init__.py            # Importaciones principales\n",
    "    ‚îú‚îÄ‚îÄ environment_setup.py   # Configuraci√≥n de ambiente NLP\n",
    "    ‚îú‚îÄ‚îÄ data_cleaner.py        # Limpieza profesional de datos\n",
    "    ‚îú‚îÄ‚îÄ nlp_processor.py       # Extracci√≥n de caracter√≠sticas NLP\n",
    "    ‚îú‚îÄ‚îÄ ml_models.py           # Entrenamiento de modelos ML\n",
    "    ‚îú‚îÄ‚îÄ visualizer.py          # Generaci√≥n de gr√°ficos\n",
    "    ‚îî‚îÄ‚îÄ progress_tracker.py    # Monitoreo de progreso\n",
    "```\n",
    "\n",
    "**Arquitectura Modular:** Cada m√≥dulo tiene una responsabilidad espec√≠fica, facilitando mantenimiento, testing y escalabilidad del sistema.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "1fbc8c4a00154002a05a0c549d6ac802",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 19467,
    "execution_start": 1754285854636,
    "source_hash": "1066cd4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modulos cargados correctamente\n"
     ]
    }
   ],
   "source": [
    "# === ANALISIS DE POPULARIDAD DE TED TALKS ===\n",
    "# Aplicacion de Extraccion de Informacion y Comparacion de Modelos ML\n",
    "\n",
    "# Importar la clase principal que controla todo el flujo\n",
    "from modules import TedTalkAnalyzer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Modulos cargados correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 1. Importaci√≥n de M√≥dulos y Configuraci√≥n Inicial\n",
    "\n",
    "### Prop√≥sito\n",
    "Esta secci√≥n inicial configura el entorno de trabajo importando la clase principal `TedTalkAnalyzer` que encapsula toda la l√≥gica del an√°lisis. Esta aproximaci√≥n modular permite mantener el c√≥digo organizado y reutilizable.\n",
    "\n",
    "### T√©cnicas Empleadas\n",
    "- **Arquitectura Modular:** Separaci√≥n de responsabilidades en diferentes m√≥dulos especializados\n",
    "- **Supresi√≥n de Warnings:** Para mantener la salida limpia durante el procesamiento\n",
    "- **Importaci√≥n Din√°mica:** Los m√≥dulos se cargan bajo demanda para optimizar memoria\n",
    "\n",
    "### M√≥dulos Principales del Sistema\n",
    "1. **`environment_setup.py`:** Gesti√≥n de dependencias y configuraci√≥n del entorno\n",
    "2. **`data_cleaner.py`:** Limpieza y preprocesamiento de datos con est√°ndares industriales\n",
    "3. **`nlp_processor.py`:** T√©cnicas avanzadas de NLP (NER, sentimientos, features textuales)\n",
    "4. **`ml_models.py`:** Entrenamiento y evaluaci√≥n de modelos de machine learning\n",
    "5. **`visualizer.py`:** Generaci√≥n de gr√°ficos y m√©tricas de rendimiento\n",
    "6. **`progress_tracker.py`:** Monitoreo en tiempo real del progreso\n",
    "\n",
    "### Beneficios de esta Arquitectura\n",
    "- **Mantenibilidad:** Cada m√≥dulo tiene una responsabilidad espec√≠fica\n",
    "- **Escalabilidad:** F√°cil agregar nuevas t√©cnicas sin afectar el c√≥digo existente\n",
    "- **Reutilizaci√≥n:** Los m√≥dulos pueden usarse en otros proyectos de NLP\n",
    "- **Testing:** Cada componente puede probarse independientemente\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "421855a6adaf4a6f90e73d9a35b69a05",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 1,
    "execution_start": 1754285874155,
    "source_hash": "65ab57b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando instancia del analizador TED Talks...\n",
      "Analizador creado correctamente\n",
      "Metodos disponibles:\n",
      "- setup_environment(): Configurar ambiente\n",
      "- load_data(): Cargar datos\n",
      "- clean_data(): Limpiar datos\n",
      "- process_nlp_features(): Procesar NLP\n",
      "- train_models(): Entrenar modelos ML\n",
      "- create_visualizations(): Crear graficos\n",
      "- run_complete_analysis(): Ejecutar todo automaticamente\n"
     ]
    }
   ],
   "source": [
    "# === CREAR INSTANCIA DEL ANALIZADOR ===\n",
    "\n",
    "print(\"Creando instancia del analizador TED Talks...\")\n",
    "\n",
    "# Crear instancia de la clase principal\n",
    "analyzer = TedTalkAnalyzer()\n",
    "\n",
    "print(\"Analizador creado correctamente\")\n",
    "print(\"Metodos disponibles:\")\n",
    "print(\"- setup_environment(): Configurar ambiente\")\n",
    "print(\"- load_data(): Cargar datos\")\n",
    "print(\"- clean_data(): Limpiar datos\")\n",
    "print(\"- process_nlp_features(): Procesar NLP\")\n",
    "print(\"- train_models(): Entrenar modelos ML\")\n",
    "print(\"- create_visualizations(): Crear graficos\")\n",
    "print(\"- run_complete_analysis(): Ejecutar todo automaticamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 2. Inicializaci√≥n del Analizador Principal\n",
    "\n",
    "### Prop√≥sito\n",
    "Crear una instancia de la clase `TedTalkAnalyzer` que actuar√° como el controlador central de todo el pipeline de an√°lisis. Esta clase implementa el patr√≥n **Factory** y **Pipeline** para gestionar el flujo completo de datos.\n",
    "\n",
    "### M√©todos Disponibles del Analizador\n",
    "\n",
    "#### M√©todos Individuales (Para Control Granular)\n",
    "- **`setup_environment()`:** Configura dependencias, descarga modelos NLP y prepara el entorno\n",
    "- **`load_data()`:** Carga y valida el dataset inicial\n",
    "- **`clean_data()`:** Aplica t√©cnicas de limpieza profesional de datos\n",
    "- **`process_nlp_features()`:** Extrae caracter√≠sticas mediante NLP avanzado\n",
    "- **`train_models()`:** Entrena y compara m√∫ltiples algoritmos de ML\n",
    "- **`create_visualizations()`:** Genera gr√°ficos y m√©tricas de rendimiento\n",
    "\n",
    "#### M√©todo Automatizado\n",
    "- **`run_complete_analysis()`:** Ejecuta todo el pipeline autom√°ticamente\n",
    "\n",
    "### Beneficios del Patr√≥n Pipeline\n",
    "1. **Trazabilidad:** Cada paso puede ejecutarse y verificarse independientemente\n",
    "2. **Debugging:** Si hay errores, se puede identificar exactamente d√≥nde ocurrieron\n",
    "3. **Experimentaci√≥n:** Permite probar diferentes configuraciones en pasos espec√≠ficos\n",
    "4. **Reproducibilidad:** El mismo pipeline garantiza resultados consistentes\n",
    "\n",
    "### Relaci√≥n con el An√°lisis Global\n",
    "Esta instancia mantendr√° todo el estado del an√°lisis (datos originales, procesados, modelos entrenados, resultados) permitiendo acceso consistente a trav√©s de todo el notebook.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "c7095e88845b4fe19708d4df2669befb",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 50249,
    "execution_start": 1754285874215,
    "source_hash": "64b5d9ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INICIO: 11:35:26\n",
      "Configurando ambiente y dependencias...\n",
      "Esto puede tomar 2-5 minutos la primera vez\n",
      "==================================================\n",
      "\n",
      "=== CONFIGURANDO AMBIENTE ===\n",
      "=== CONFIGURACION DEL AMBIENTE ===\n",
      "Tiempo estimado: 2-5 minutos\n",
      "\n",
      "PASO 1/3: Instalando 8 paquetes esenciales...\n",
      "  [1/8] Instalando pandas>=1.3.0... OK\n",
      "  [2/8] Instalando numpy>=2.0.0... OK\n",
      "  [3/8] Instalando scikit-learn>=1.0.0... OK\n",
      "  [4/8] Instalando matplotlib>=3.4.0... OK\n",
      "  [5/8] Instalando seaborn>=0.11.0... OK\n",
      "  [6/8] Instalando nltk>=3.7... OK\n",
      "  [7/8] Instalando textblob>=0.17.0... OK\n",
      "  [8/8] Instalando tqdm>=4.64.0... OK\n",
      "\n",
      "Paquetes esenciales: 8/8 instalados\n",
      "\n",
      "PASO 2/3: Instalando 3 paquetes opcionales...\n",
      "  [1/3] Instalando plotly>=5.0.0... OK\n",
      "  [2/3] Instalando spacy>=3.4.0... OK\n",
      "  [3/3] Instalando wordcloud>=1.8.0... OK\n",
      "\n",
      "Paquetes opcionales: 3/3 instalados\n",
      "\n",
      "PASO 3/3: Configurando modelos de NLP...\n",
      "  Descargando datos NLTK...OK\n",
      "  Verificando spaCy..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OK\n",
      "\n",
      "CONFIGURACION COMPLETADA\n",
      "========================================\n",
      "‚úì GPU disponible: Tesla T4\n",
      "‚úì spaCy cargado correctamente\n",
      "‚úì NLTK configurado correctamente\n",
      "Ambiente configurado correctamente\n",
      "\n",
      "Tiempo total: 38.7 segundos\n",
      "COMPLETADO: 11:36:04\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURACION DEL AMBIENTE ===\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"INICIO:\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "print(\"Configurando ambiente y dependencias...\")\n",
    "print(\"Esto puede tomar 2-5 minutos la primera vez\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configurar ambiente usando el metodo del analizador\n",
    "start_time = time.time()\n",
    "analyzer.setup_environment()\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed = end_time - start_time\n",
    "print(f\"\\nTiempo total: {elapsed:.1f} segundos\")\n",
    "print(f\"COMPLETADO:\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 3. Configuraci√≥n del Ambiente y Dependencias\n",
    "\n",
    "### Prop√≥sito\n",
    "Preparar todo el entorno computacional necesario para el an√°lisis avanzado de NLP. Este paso es cr√≠tico porque descarga y configura modelos pre-entrenados, librer√≠as especializadas y datos auxiliares.\n",
    "\n",
    "### Componentes que se Configuran\n",
    "\n",
    "#### Librer√≠as de NLP\n",
    "- **NLTK:** Descarga de corpora (stopwords, punkt tokenizer, VADER lexicon)\n",
    "- **spaCy:** Modelo `en_core_web_sm` para NER y an√°lisis sint√°ctico\n",
    "- **Hugging Face:** Modelos pre-entrenados para an√°lisis de sentimientos avanzado\n",
    "\n",
    "#### Optimizaciones de Rendimiento\n",
    "- **Cach√© Inteligente:** Evita descargas repetidas si los modelos ya existen\n",
    "- **Verificaci√≥n de Integridad:** Valida que todos los modelos se descargaron correctamente\n",
    "- **Gesti√≥n de Memoria:** Configuraci√≥n √≥ptima para procesamiento de texto grande\n",
    "\n",
    "### T√©cnicas Empleadas\n",
    "\n",
    "#### ¬øPor qu√© este paso toma tiempo?\n",
    "La primera ejecuci√≥n descarga varios GB de datos:\n",
    "- Modelo spaCy en_core_web_sm (~50MB)\n",
    "- Corpora de NLTK (~200MB)\n",
    "- Vocabularios y embeddings pre-entrenados\n",
    "\n",
    "#### Estrategia de Cach√©\n",
    "El sistema implementa cach√© persistente que:\n",
    "1. Verifica si los modelos ya est√°n instalados\n",
    "2. Solo descarga componentes faltantes\n",
    "3. Valida integridad de archivos existentes\n",
    "\n",
    "### Resultados Esperados\n",
    "-  Todos los modelos NLP listos para usar\n",
    "-  Configuraci√≥n optimizada de memoria\n",
    "-  Verificaci√≥n de compatibilidad de versiones\n",
    "- Ô∏è Tiempo t√≠pico: 2-5 minutos (primera vez), <30 segundos (subsecuentes)\n",
    "\n",
    "### Relaci√≥n con el An√°lisis Global\n",
    "Sin este paso, las t√©cnicas avanzadas de NLP no funcionar√≠an. Es la base que permite:\n",
    "- Reconocimiento preciso de entidades nombradas\n",
    "- An√°lisis de sentimientos con modelos state-of-the-art\n",
    "- Tokenizaci√≥n y lemmatizaci√≥n eficientes\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "ea1bbcae76a446a08d44ec5f68a6190f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 1125,
    "execution_start": 1754286173402,
    "source_hash": "f2aee897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset ted_talks_en.csv...\n",
      "\n",
      "=== CARGANDO DATASET: ted_talks_en.csv ===\n",
      "Dataset cargado: 4005 filas x 19 columnas\n",
      "\n",
      "Columnas disponibles:\n",
      " 1. talk_id\n",
      " 2. title\n",
      " 3. speaker_1\n",
      " 4. all_speakers\n",
      " 5. occupations\n",
      " 6. about_speakers\n",
      " 7. views\n",
      " 8. recorded_date\n",
      " 9. published_date\n",
      "10. event\n",
      "11. native_lang\n",
      "12. available_lang\n",
      "13. comments\n",
      "14. duration\n",
      "15. topics\n",
      "16. related_talks\n",
      "17. url\n",
      "18. description\n",
      "19. transcript\n",
      "Dataset cargado exitosamente\n",
      "Filas: 4,005\n",
      "Columnas: 19\n",
      "Memoria utilizada: 81.26 MB\n",
      "\n",
      "Columnas disponibles:\n",
      "  1. talk_id\n",
      "  2. title\n",
      "  3. speaker_1\n",
      "  4. all_speakers\n",
      "  5. occupations\n",
      "  6. about_speakers\n",
      "  7. views\n",
      "  8. recorded_date\n",
      "  9. published_date\n",
      "  10. event\n",
      "  11. native_lang\n",
      "  12. available_lang\n",
      "  13. comments\n",
      "  14. duration\n",
      "  15. topics\n",
      "  16. related_talks\n",
      "  17. url\n",
      "  18. description\n",
      "  19. transcript\n"
     ]
    }
   ],
   "source": [
    "# === CARGA DE DATOS ===\n",
    "\n",
    "print(\"Cargando dataset ted_talks_en.csv...\")\n",
    "\n",
    "# Cargar datos usando el metodo del analizador\n",
    "analyzer.load_data('ted_talks_en.csv')\n",
    "\n",
    "# Mostrar informacion basica\n",
    "if hasattr(analyzer, 'data') and analyzer.data is not None:\n",
    "    print(f\"Dataset cargado exitosamente\")\n",
    "    print(f\"Filas: {analyzer.data.shape[0]:,}\")\n",
    "    print(f\"Columnas: {analyzer.data.shape[1]}\")\n",
    "    print(f\"Memoria utilizada: {analyzer.data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Mostrar primeras columnas\n",
    "    print(\"\\nColumnas disponibles:\")\n",
    "    for i, col in enumerate(analyzer.data.columns):\n",
    "        print(f\"  {i+1}. {col}\")\n",
    "else:\n",
    "    print(\"ERROR: No se pudo cargar el dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 4. Carga y An√°lisis Exploratorio del Dataset\n",
    "\n",
    "### Prop√≥sito\n",
    "Cargar el dataset `ted_talks_en.csv` y realizar una exploraci√≥n inicial para entender la estructura, calidad y caracter√≠sticas de los datos. Este paso es fundamental para planificar estrategias de limpieza y procesamiento.\n",
    "\n",
    "### Caracter√≠sticas del Dataset TED Talks\n",
    "\n",
    "#### Informaci√≥n Esperada\n",
    "El dataset contiene informaci√≥n sobre charlas TED con las siguientes columnas principales:\n",
    "- **Identificadores:** ID, t√≠tulo, speaker\n",
    "- **M√©tricas de Popularidad:** views, ratings, comments\n",
    "- **Contenido Textual:** transcript (transcripci√≥n completa), description\n",
    "- **Metadatos:** date, duration, tags, related_talks\n",
    "\n",
    "#### T√©cnicas de Validaci√≥n Aplicadas\n",
    "1. **Verificaci√≥n de Integridad:** Confirmar que el archivo existe y es legible\n",
    "2. **An√°lisis de Dimensiones:** N√∫mero de filas y columnas\n",
    "3. **Evaluaci√≥n de Memoria:** Calcular uso de memoria para optimizar procesamiento\n",
    "4. **Inspecci√≥n de Tipos de Datos:** Identificar columnas num√©ricas, texto, fechas\n",
    "\n",
    "### M√©tricas de Calidad de Datos\n",
    "\n",
    "#### ¬øPor qu√© es Importante la Exploraci√≥n Inicial?\n",
    "- **Detectar Problemas Tempranos:** Valores faltantes, duplicados, inconsistencias\n",
    "- **Planificar Estrategias:** Decidir t√©cnicas de limpieza apropiadas\n",
    "- **Estimar Recursos:** Calcular tiempo y memoria necesarios para procesamiento\n",
    "\n",
    "#### Informaci√≥n de Salida Clave\n",
    "- **N√∫mero de Registros:** Total de TED Talks disponibles\n",
    "- **Dimensionalidad:** Cantidad de caracter√≠sticas disponibles\n",
    "- **Uso de Memoria:** Para planificar operaciones eficientes\n",
    "- **Columnas Disponibles:** Lista completa de variables del dataset\n",
    "\n",
    "### Resultados Esperados\n",
    "-  Confirmaci√≥n exitosa de carga del dataset\n",
    "-  Estad√≠sticas b√°sicas de dimensiones\n",
    "-  Informaci√≥n de uso de memoria\n",
    "-  Inventario completo de columnas disponibles\n",
    "\n",
    "### Relaci√≥n con el An√°lisis Global\n",
    "Esta carga inicial alimentar√° todos los pasos subsecuentes. La calidad y completitud de este dataset determinar√°:\n",
    "- Qu√© t√©cnicas de NLP pueden aplicarse\n",
    "- Cu√°ntos datos tendremos para entrenamiento\n",
    "- Qu√© variables pueden usarse como predictores de popularidad\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "3502e50b80ca4a468e9a9a09f436fe09",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 4925,
    "execution_start": 1754286262535,
    "source_hash": "1b81bd79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando limpieza profesional de datos...\n",
      "\n",
      "=== LIMPIANDO DATOS ===\n",
      "Iniciando: Iniciando limpieza\n",
      "Tiempo de inicio: 11:36:06\n",
      "==================================================\n",
      "[11:36:06] Dataset original: 4005 filas x 19 columnas\n",
      "[1/4] (25.0%) Eliminando outliers con m√©todo IQR... OK\n",
      "Analizando distribuci√≥n de 'views'...\n",
      "   - Q1 (25%): 882,069\n",
      "   - Q3 (75%): 2,133,110\n",
      "   - IQR: 1,251,041\n",
      "   - L√≠mite inferior: -994,492\n",
      "   - L√≠mite superior: 4,009,672\n",
      "   - Outliers identificados: 393 (9.81%)\n",
      "[11:36:07] Dataset despu√©s de eliminar outliers: 3612 filas\n",
      "[2/4] (50.0%) Limpiando datos textuales... OK\n",
      "[11:36:07] Procesando columna: title\n",
      " - Valores vac√≠os: 0\n",
      " - Longitud promedio: 38.4 caracteres\n",
      "[11:36:07] Procesando columna: description\n",
      " - Valores vac√≠os: 0\n",
      " - Longitud promedio: 352.8 caracteres\n",
      "[11:36:07] Procesando columna: transcript\n",
      " - Valores vac√≠os: 0\n",
      " - Longitud promedio: 9870.6 caracteres\n",
      "[11:36:13] Procesadas 3 columnas de texto\n",
      "[3/4] (75.0%) Creando categor√≠as de popularidad... OK\n",
      " Analizando distribuci√≥n de popularidad...\n",
      "   Umbrales de popularidad:\n",
      "     - Bajo: hasta 695,206 views\n",
      "     - Medio Bajo: hasta 1,111,279 views\n",
      "     - Medio: hasta 1,470,199 views\n",
      "     - Medio Alto: hasta 1,994,938 views\n",
      "     - Alto: hasta 4,006,448 views\n",
      "\n",
      "   Distribuci√≥n de categor√≠as:\n",
      "     - Bajo: 723 (20.0%)\n",
      "     - Medio Bajo: 722 (20.0%)\n",
      "     - Medio: 722 (20.0%)\n",
      "     - Medio Alto: 722 (20.0%)\n",
      "     - Alto: 723 (20.0%)\n",
      "[4/4] (100.0%) Validando dataset limpio... OK\n",
      "[11:36:13] Dimensiones finales: 3612 filas x 24 columnas\n",
      "[11:36:13] Filas eliminadas: 393 (9.81%)\n",
      "[11:36:13] Puntuaci√≥n de calidad: 7.85/10\n",
      "\n",
      "==================================================\n",
      "Estado: Limpieza de datos completada\n",
      "Tiempo total: 6.6 segundos\n",
      "Finalizado: 11:36:13\n",
      "Tiempo promedio por paso: 1.7s\n",
      "Datos limpiados correctamente\n",
      "\n",
      "Resultados de la limpieza:\n",
      "  Filas originales: 4,005\n",
      "  Filas despues de limpieza: 3,612\n",
      "  Filas eliminadas: 393 (9.8%)\n",
      "\n",
      "Categorias de popularidad:\n",
      "  Bajo: 723 videos\n",
      "  Medio Bajo: 722 videos\n",
      "  Medio: 722 videos\n",
      "  Medio Alto: 722 videos\n",
      "  Alto: 723 videos\n"
     ]
    }
   ],
   "source": [
    "# === LIMPIEZA DE DATOS ===\n",
    "\n",
    "print(\"Aplicando limpieza profesional de datos...\")\n",
    "\n",
    "# Limpiar datos usando el metodo del analizador\n",
    "analyzer.clean_data()\n",
    "\n",
    "# Mostrar resultados de la limpieza\n",
    "if hasattr(analyzer, 'df_clean') and analyzer.df_clean is not None:\n",
    "    original_count = analyzer.data.shape[0]\n",
    "    clean_count = analyzer.df_clean.shape[0]\n",
    "    removed_count = original_count - clean_count\n",
    "    \n",
    "    print(f\"\\nResultados de la limpieza:\")\n",
    "    print(f\"  Filas originales: {original_count:,}\")\n",
    "    print(f\"  Filas despues de limpieza: {clean_count:,}\")\n",
    "    print(f\"  Filas eliminadas: {removed_count:,} ({removed_count/original_count*100:.1f}%)\")\n",
    "    \n",
    "    # Mostrar categorias de popularidad creadas\n",
    "    if 'popularity_category' in analyzer.df_clean.columns:\n",
    "        print(\"\\nCategorias de popularidad:\")\n",
    "        categories = analyzer.df_clean['popularity_category'].value_counts().sort_index()\n",
    "        for category, count in categories.items():\n",
    "            print(f\"  {category}: {count:,} videos\")\n",
    "            \n",
    "    # Mostrar calidad de datos\n",
    "    if 'df_cleaning' in analyzer.results:\n",
    "        quality_score = analyzer.results['df_cleaning']['quality_results']['quality_score']\n",
    "        print(f\"\\nPuntuacion de calidad de datos: {quality_score:.2f}/10\")\n",
    "else:\n",
    "    print(\"ERROR: No se pudo limpiar el dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 5. Limpieza Profesional de Datos\n",
    "\n",
    "### Prop√≥sito\n",
    "Aplicar t√©cnicas de limpieza de datos siguiendo est√°ndares industriales similares a los usados por ingenieros en Amazon y Google. Esta fase es cr√≠tica porque datos de mala calidad producen modelos de baja calidad.\n",
    "\n",
    "### T√©cnicas de Limpieza Implementadas\n",
    "\n",
    "#### 1. Eliminaci√≥n de Outliers Extremos\n",
    "- **M√©todo IQR (Interquartile Range):** Identifica valores at√≠picos en la columna 'views'\n",
    "- **¬øPor qu√© eliminar outliers?** Videos con views extremadamente altos (virales) o bajos (errores) pueden sesgar el modelo\n",
    "- **C√°lculo:** Q1 - 1.5√óIQR y Q3 + 1.5√óIQR como l√≠mites\n",
    "\n",
    "#### 2. Manejo de Valores Faltantes\n",
    "- **Estrategia Adaptativa:** Diferentes t√©cnicas seg√∫n el tipo de columna\n",
    "- **Texto:** Imputaci√≥n con \"Unknown\" o eliminaci√≥n si es cr√≠tico\n",
    "- **Num√©rico:** Mediana para datos sesgados, media para distribuciones normales\n",
    "- **Categ√≥rico:** Moda o nueva categor√≠a \"Missing\"\n",
    "\n",
    "#### 3. Normalizaci√≥n y Estandarizaci√≥n\n",
    "- **Limpieza de Texto:** Eliminaci√≥n de caracteres especiales, normalizaci√≥n de encoding\n",
    "- **Fechas:** Conversi√≥n a formato est√°ndar datetime\n",
    "- **Strings:** Normalizaci√≥n de may√∫sculas/min√∫sculas\n",
    "\n",
    "### Creaci√≥n de Variable Objetivo\n",
    "\n",
    "#### Categorizaci√≥n de Popularidad\n",
    "El sistema convierte la variable continua 'views' en categor√≠as discretas:\n",
    "\n",
    "1. **\"bajo\":** Videos con pocas visualizaciones\n",
    "2. **\"medio bajo\":** Popularidad limitada\n",
    "3. **\"medio\":** Popularidad promedio\n",
    "4. **\"medio alto\":** Popular\n",
    "5. **\"alto\":** Muy popular\n",
    "\n",
    "#### ¬øPor qu√© Categorizar en lugar de Regresi√≥n?\n",
    "- **Interpretabilidad:** Es m√°s f√°cil entender \"popular vs no popular\"\n",
    "- **Robustez:** Menos sensible a outliers extremos\n",
    "- **Aplicabilidad:** Los stakeholders prefieren clasificaciones claras\n",
    "\n",
    "### M√©tricas de Calidad\n",
    "\n",
    "#### Puntuaci√≥n de Calidad de Datos (Scale 1-10)\n",
    "El sistema calcula un score basado en:\n",
    "- **Completitud:** Porcentaje de valores no faltantes\n",
    "- **Consistencia:** Coherencia entre variables relacionadas\n",
    "- **Validez:** Valores dentro de rangos esperados\n",
    "- **Precisi√≥n:** Detecci√≥n de errores evidentes\n",
    "\n",
    "### Resultados Esperados\n",
    "-  Reducci√≥n controlada del dataset (t√≠picamente 5-15% de registros eliminados)\n",
    "-  5 categor√≠as balanceadas de popularidad\n",
    "-  Score de calidad de datos > 7.0/10\n",
    "-  Dataset libre de valores faltantes cr√≠ticos\n",
    "\n",
    "### Relaci√≥n con el An√°lisis Global\n",
    "Esta limpieza determina directamente:\n",
    "- **Calidad del Modelo:** Datos limpios = modelos m√°s precisos\n",
    "- **Representatividad:** Mantener diversidad mientras eliminamos ruido\n",
    "- **Eficiencia Computacional:** Menos datos irrelevantes = entrenamiento m√°s r√°pido\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a62b3e99d499449a8f1e5658cb1acfbb",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 22370,
    "execution_start": 1754286327855,
    "source_hash": "7f506687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando tecnicas de extraccion de informacion...\n",
      "Procesando: sentimientos, entidades nombradas, caracteristicas textuales\n",
      "\n",
      "=== PROCESANDO CARACTER√çSTICAS NLP ===\n",
      "Iniciando: Iniciando extracci√≥n de caracter√≠sticas NLP\n",
      "Tiempo de inicio: 11:36:13\n",
      "==================================================\n",
      "[11:36:13] Procesando columna: transcript_clean\n",
      "[1/5] (20.0%) Cargando modelos de NLP... OK\n",
      "[2/5] (40.0%) Preparando muestra de datos... OK\n",
      "[11:36:14] Procesando muestra de 100 textos para velocidad...\n",
      "[3/5] (60.0%) Analizando sentimientos con TextBlob... OK\n",
      "[11:36:14] Analizando polaridad y subjetividad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deca1d1eb1744fbd938460964bd4bd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentimientos:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:36:27] An√°lisis de sentimientos completado\n",
      "[4/5] (80.0%) Extrayendo caracter√≠sticas textuales... OK\n",
      "[11:36:27] Calculando longitud, palabras, oraciones...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a442854dac494f2aa7d04dd92f5f5757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Caracter√≠sticas:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:36:31] Caracter√≠sticas textuales extra√≠das\n",
      "[5/5] (100.0%) Identificando entidades nombradas... OK\n",
      "Procesando 100 textos para entidades nombradas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eedbf6c23a640c492fe9e9b70f3a7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entidades:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === EXTRACCION DE INFORMACION CON NLP ===\n",
    "\n",
    "print(\"Aplicando tecnicas de extraccion de informacion...\")\n",
    "print(\"Procesando: sentimientos, entidades nombradas, caracteristicas textuales\")\n",
    "\n",
    "# Procesar caracteristicas NLP usando el metodo del analizador\n",
    "analyzer.process_nlp_features(text_column='transcript_clean')\n",
    "\n",
    "# Mostrar caracteristicas extraidas\n",
    "# Fix: Check if 'df_processed' attribute exists before accessing it\n",
    "if hasattr(analyzer, 'df_processed') and analyzer.df_processed is not None:\n",
    "    print(f\"\\nExtraccion de informacion completada\")\n",
    "    print(f\"Dataset procesado: {analyzer.df_processed.shape}\")\n",
    "    \n",
    "    # Identificar caracteristicas NLP creadas\n",
    "    nlp_features = [col for col in analyzer.df_processed.columns if \n",
    "                   col.startswith(('sentiment_', 'text_', 'person_', 'org_', 'gpe_'))]\n",
    "    \n",
    "    print(f\"\\nCaracteristicas NLP extraidas: {len(nlp_features)}\")\n",
    "    print(\"Tipos de informacion extraida:\")\n",
    "    \n",
    "    # Agrupar por tipo\n",
    "    sentiment_features = [f for f in nlp_features if f.startswith('sentiment_')]\n",
    "    text_features = [f for f in nlp_features if f.startswith('text_')]\n",
    "    entity_features = [f for f in nlp_features if f.startswith(('person_', 'org_', 'gpe_'))]\n",
    "    \n",
    "    if sentiment_features:\n",
    "        print(f\"  Analisis de sentimientos: {len(sentiment_features)} caracteristicas\")\n",
    "    if text_features:\n",
    "        print(f\"  Caracteristicas textuales: {len(text_features)} caracteristicas\") \n",
    "    if entity_features:\n",
    "        print(f\"  Entidades nombradas: {len(entity_features)} caracteristicas\")\n",
    "        \n",
    "    # Mostrar estadisticas de muestra procesada\n",
    "    if 'nlp_processing' in analyzer.results:\n",
    "        word_frequencies = analyzer.results['nlp_processing']['word_frequencies']\n",
    "        print(f\"\\nMuestra procesada: {word_frequencies} registros\")\n",
    "else:\n",
    "    print(\"ERROR: No se pudo procesar las caracteristicas NLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd;color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 6. Extracci√≥n de Informaci√≥n mediante T√©cnicas Avanzadas de NLP\n",
    "\n",
    "### Prop√≥sito\n",
    "Aplicar t√©cnicas state-of-the-art de procesamiento de lenguaje natural para extraer caracter√≠sticas significativas de las transcripciones de TED Talks. Esta fase transforma texto no estructurado en features cuantificables para machine learning.\n",
    "\n",
    "### T√©cnicas de NLP Implementadas\n",
    "\n",
    "#### 1. Named Entity Recognition (NER)\n",
    "- **Modelo:** spaCy `en_core_web_sm` con arquitectura transformer\n",
    "- **Entidades Extra√≠das:**\n",
    "  - **PERSON:** Nombres de personas mencionadas (expertos, autores, figuras hist√≥ricas)\n",
    "  - **ORG:** Organizaciones (empresas, universidades, ONGs)\n",
    "  - **GPE:** Ubicaciones geopol√≠ticas (pa√≠ses, ciudades)\n",
    "- **¬øPor qu√© es importante?** Videos que mencionan muchas organizaciones prestigiosas tienden a ser m√°s populares\n",
    "\n",
    "#### 2. An√°lisis de Sentimientos Multi-Dimensional\n",
    "- **VADER Sentiment:** Optimizado para texto de redes sociales y informal\n",
    "- **Caracter√≠sticas Extra√≠das:**\n",
    "  - **Polaridad:** Escala -1 (muy negativo) a +1 (muy positivo)\n",
    "  - **Subjetividad:** Escala 0 (objetivo/factual) a 1 (subjetivo/opini√≥n)\n",
    "- **Hip√≥tesis:** TED Talks con tono positivo pero contenido objetivo tienen mayor engagement\n",
    "\n",
    "#### 3. Caracter√≠sticas Textuales Avanzadas\n",
    "- **Longitud y Complejidad:**\n",
    "  - N√∫mero total de palabras\n",
    "  - Palabras √∫nicas (vocabulario diverso)\n",
    "  - Promedio de palabras por oraci√≥n\n",
    "- **An√°lisis L√©xico:**\n",
    "  - Frecuencia de stopwords (indicador de fluidez)\n",
    "  - Densidad de contenido (palabras significativas vs funcionales)\n",
    "\n",
    "### Metodolog√≠a de Procesamiento\n",
    "\n",
    "#### ¬øPor qu√© usar spaCy en lugar de NLTK?\n",
    "- **Rendimiento:** 10x m√°s r√°pido para NER en textos largos\n",
    "- **Precisi√≥n:** Modelos pre-entrenados en corpora masivos\n",
    "- **Escalabilidad:** Dise√±ado para procesamiento en producci√≥n\n",
    "\n",
    "#### Pipeline de Procesamiento de Texto\n",
    "1. **Limpieza:** Normalizaci√≥n de encoding, eliminaci√≥n de HTML/XML\n",
    "2. **Tokenizaci√≥n:** Divisi√≥n inteligente en palabras y oraciones\n",
    "3. **Lemmatizaci√≥n:** Reducci√≥n a formas base (running ‚Üí run)\n",
    "4. **NER:** Identificaci√≥n y clasificaci√≥n de entidades\n",
    "5. **Sentimientos:** An√°lisis emocional del contenido\n",
    "\n",
    "### Optimizaciones de Rendimiento\n",
    "\n",
    "#### Procesamiento por Lotes\n",
    "- **Batch Processing:** Procesa m√∫ltiples textos simult√°neamente\n",
    "- **Gesti√≥n de Memoria:** Liberaci√≥n autom√°tica de memoria entre lotes\n",
    "- **Progreso en Tiempo Real:** Barra de progreso para monitorear avance\n",
    "\n",
    "#### Cach√© Inteligente\n",
    "Si los datos de entrada no han cambiado, el sistema reutiliza caracter√≠sticas previamente extra√≠das.\n",
    "\n",
    "### Resultados Esperados\n",
    "\n",
    "#### Nuevas Caracter√≠sticas Creadas\n",
    "- **Sentimientos:** `sentiment_polarity`, `sentiment_subjectivity`\n",
    "- **Entidades:** `person_count`, `org_count`, `gpe_count`\n",
    "- **Texto:** `text_length`, `text_unique_words`, `text_avg_sentence_length`\n",
    "\n",
    "#### Estad√≠sticas T√≠picas\n",
    "- Videos procesan ~5-15 caracter√≠sticas NLP adicionales\n",
    "- Tiempo de procesamiento: ~2-5 minutos para 1000 videos\n",
    "- Incremento de dimensionalidad: +10-20 columnas\n",
    "\n",
    "### Relaci√≥n con el An√°lisis Global\n",
    "\n",
    "#### Hip√≥tesis a Validar\n",
    "1. **TED Talks que mencionan m√°s organizaciones prestigiosas son m√°s populares**\n",
    "2. **Contenido con tono positivo pero objetivo atrae m√°s audiencia**\n",
    "3. **Transcripciones de longitud media tienen mejor engagement**\n",
    "4. **Diversidad de vocabulario correlaciona con popularidad**\n",
    "\n",
    "Estas caracter√≠sticas alimentar√°n directamente los modelos de machine learning para predecir popularidad.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:03:25.803244Z",
     "start_time": "2025-08-03T07:03:25.143615Z"
    },
    "cell_id": "8c9511cdd7ca47c9988212864a9ff788",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 38697,
    "execution_start": 1754286482018,
    "source_hash": "a6a134a7"
   },
   "outputs": [],
   "source": [
    "# === ENTRENAMIENTO Y COMPARACION DE MODELOS ML ===\n",
    "\n",
    "print(\"Entrenando y comparando modelos de Machine Learning...\")\n",
    "print(\"Objetivo: F1-score > 0.78\")\n",
    "\n",
    "# Entrenar modelos usando el metodo del analizador\n",
    "analyzer.train_models(text_column='transcript_clean', target_column='popularity_numeric')\n",
    "\n",
    "# Mostrar resultados de los modelos\n",
    "if 'machine_learning' in analyzer.results:\n",
    "    # Check if 'evaluation_results' key exists before accessing it\n",
    "    if 'evaluation_results' in analyzer.results['machine_learning']:\n",
    "        ml_results = analyzer.results['machine_learning']['evaluation_results']\n",
    "        models_trained = analyzer.results['machine_learning']['models_trained']\n",
    "        \n",
    "        print(\"\\nRESULTADOS DE MODELOS:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Mostrar resultados de cada modelo\n",
    "        for model_name, results in ml_results.items():\n",
    "            if results is not None:\n",
    "                print(f\"\\n{model_name}:\")\n",
    "                print(f\"  Accuracy:  {results['accuracy']:.4f}\")\n",
    "                print(f\"  Precision: {results['precision']:.4f}\")\n",
    "                print(f\"  Recall:    {results['recall']:.4f}\")\n",
    "                print(f\"  F1-Score:  {results['f1_score']:.4f}\")\n",
    "                \n",
    "                # Verificar si cumple objetivo\n",
    "                objetivo_cumplido = \"SI\" if results['f1_score'] > 0.78 else \"NO\"\n",
    "                print(f\"  Objetivo F1>0.78: {objetivo_cumplido}\")\n",
    "        \n",
    "        # Identificar mejor modelo\n",
    "        best_model_name, best_model, best_score = analyzer.results['machine_learning']['best_model']\n",
    "        print(f\"\\nMEJOR MODELO: {best_model_name}\")\n",
    "        print(f\"F1-Score: {best_score:.4f}\")\n",
    "        \n",
    "        if best_score > 0.78:\n",
    "            print(\"Objetivo cumplido! F1-Score > 0.78\")\n",
    "        else:\n",
    "            print(\"Objetivo no cumplido. Considerar mas datos o mejores caracteristicas.\")\n",
    "            \n",
    "        # Guardar el mejor modelo para referencia\n",
    "        analyzer.best_model = best_model_name\n",
    "        analyzer.best_f1_score = best_score\n",
    "    else:\n",
    "        print(\"ERROR: 'evaluation_results' no est√É¬° disponible en los resultados de machine_learning\")\n",
    "else:\n",
    "    print(\"ERROR: No se pudieron entrenar los modelos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 7. Entrenamiento y Comparaci√≥n de Modelos de Machine Learning\n",
    "\n",
    "### Prop√≥sito\n",
    "Entrenar m√∫ltiples algoritmos de machine learning para predecir la popularidad de TED Talks y seleccionar el mejor modelo basado en m√©tricas de rendimiento. El objetivo es superar un F1-score de 0.78.\n",
    "\n",
    "### Algoritmos de Machine Learning Implementados\n",
    "\n",
    "#### 1. Random Forest Classifier\n",
    "- **Fortalezas:** Robusto a outliers, maneja caracter√≠sticas mixtas (texto + num√©rico)\n",
    "- **Hiperpar√°metros:** n_estimators=100, max_depth=10, random_state=42\n",
    "- **¬øPor qu√© Random Forest?** Excelente para datos textuales con muchas caracter√≠sticas\n",
    "\n",
    "#### 2. Support Vector Machine (SVM)\n",
    "- **Kernel:** RBF (Radial Basis Function) para capturar relaciones no lineales\n",
    "- **Fortalezas:** Efectivo en espacios de alta dimensionalidad (t√≠pico en NLP)\n",
    "- **Hiperpar√°metros:** C=1.0, gamma='scale'\n",
    "\n",
    "#### 3. Gradient Boosting Classifier\n",
    "- **Implementaci√≥n:** XGBoost para optimizaci√≥n avanzada\n",
    "- **Fortalezas:** Excelente rendimiento en competencias de datos\n",
    "- **Estrategia:** Boosting secuencial corrige errores de modelos previos\n",
    "\n",
    "### Metodolog√≠a de Entrenamiento\n",
    "\n",
    "#### Preparaci√≥n de Caracter√≠sticas\n",
    "1. **Vectorizaci√≥n TF-IDF:** Convierte texto en vectores num√©ricos\n",
    "   - **¬øQu√© es TF-IDF?** Term Frequency-Inverse Document Frequency\n",
    "   - **Configuraci√≥n:** max_features=5000, stop_words='english'\n",
    "   - **Beneficio:** Resalta palabras importantes espec√≠ficas de cada documento\n",
    "\n",
    "2. **Combinaci√≥n de Features:**\n",
    "   - Caracter√≠sticas textuales (TF-IDF del transcript)\n",
    "   - Caracter√≠sticas NLP (sentimientos, entidades, m√©tricas textuales)\n",
    "   - Caracter√≠sticas num√©ricas originales (duration, ratings)\n",
    "\n",
    "#### Divisi√≥n de Datos\n",
    "- **Training Set:** 80% de los datos para entrenamiento\n",
    "- **Test Set:** 20% para evaluaci√≥n final imparcial\n",
    "- **Estratificaci√≥n:** Mantiene proporci√≥n de categor√≠as de popularidad\n",
    "\n",
    "### M√©tricas de Evaluaci√≥n\n",
    "\n",
    "#### ¬øPor qu√© F1-Score como M√©trica Principal?\n",
    "- **Balance:** Combina precisi√≥n y recall\n",
    "- **Manejo de Clases Desbalanceadas:** M√°s robusto que accuracy simple\n",
    "- **Interpretabilidad:** F√°cil de entender para stakeholders\n",
    "\n",
    "#### M√©tricas Complementarias\n",
    "1. **Accuracy:** Porcentaje de predicciones correctas\n",
    "2. **Precision:** De las predicciones positivas, cu√°ntas fueron correctas\n",
    "3. **Recall:** De los casos positivos reales, cu√°ntos detectamos\n",
    "4. **F1-Score:** Media harm√≥nica de precision y recall\n",
    "\n",
    "### Estrategia de Optimizaci√≥n\n",
    "\n",
    "#### Validaci√≥n Cruzada\n",
    "- **K-Fold CV:** 5 pliegues para estimaci√≥n robusta de rendimiento\n",
    "- **Grid Search:** B√∫squeda sistem√°tica de mejores hiperpar√°metros\n",
    "- **Estratificada:** Mantiene distribuci√≥n de clases en cada fold\n",
    "\n",
    "#### Selecci√≥n del Mejor Modelo\n",
    "1. Entrenar todos los algoritmos con los mismos datos\n",
    "2. Comparar F1-scores en validaci√≥n cruzada\n",
    "3. Seleccionar el modelo con mejor rendimiento promedio\n",
    "4. Evaluaci√≥n final en test set independiente\n",
    "\n",
    "### Resultados Esperados\n",
    "\n",
    "#### Benchmark de Rendimiento\n",
    "- **Meta Principal:** F1-Score > 0.78\n",
    "- **Accuracy Esperado:** > 0.75\n",
    "- **Tiempo de Entrenamiento:** 3-10 minutos dependiendo del tama√±o de datos\n",
    "\n",
    "#### An√°lisis de Resultados\n",
    "Para cada modelo se reporta:\n",
    "- M√©tricas de rendimiento completas\n",
    "- Cumplimiento del objetivo F1 > 0.78\n",
    "- Identificaci√≥n del mejor modelo overall\n",
    "\n",
    "### Relaci√≥n con el An√°lisis Global\n",
    "\n",
    "#### Validaci√≥n de Hip√≥tesis\n",
    "Los resultados del modelo permitir√°n confirmar:\n",
    "- Qu√© caracter√≠sticas NLP son m√°s predictivas de popularidad\n",
    "- Si las t√©cnicas de extracci√≥n de informaci√≥n mejoran la predicci√≥n\n",
    "- Cu√°l combinaci√≥n de algoritmo + caracter√≠sticas alcanza el mejor rendimiento\n",
    "\n",
    "#### Importancia de Caracter√≠sticas\n",
    "El mejor modelo revelar√° qu√© factores realmente determinan la popularidad de TED Talks.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T20:23:05.092879Z",
     "start_time": "2025-08-02T20:23:05.090755Z"
    },
    "cell_id": "cc67a9ef556e4065b9f55820612dba5d",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 0,
    "execution_start": 1754286520785,
    "source_hash": "176a87db"
   },
   "outputs": [],
   "source": [
    "# === METRICAS DE RENDIMIENTO Y VISUALIZACIONES ===\n",
    "\n",
    "print(\"Generando metricas de rendimiento y visualizaciones...\")\n",
    "\n",
    "# Crear visualizaciones usando el metodo del analizador\n",
    "analyzer.create_visualizations()\n",
    "\n",
    "# Mostrar informacion sobre las visualizaciones creadas\n",
    "if 'visualizations' in analyzer.results:\n",
    "    print(\"\\nVisualizaciones creadas exitosamente:\")\n",
    "    \n",
    "    # Si hay un clasificador disponible, mostrar importancia de caracteristicas\n",
    "    if hasattr(analyzer, 'best_model') and 'machine_learning' in analyzer.results:\n",
    "        models_trained = analyzer.results['machine_learning']['models_trained']\n",
    "        \n",
    "        print(f\"\\nImportancia de caracteristicas del mejor modelo ({analyzer.best_model}):\")\n",
    "        try:\n",
    "            feature_importance = models_trained.get_feature_importance(analyzer.best_model, top_n=10)\n",
    "            for i, (feature, importance) in enumerate(feature_importance, 1):\n",
    "                print(f\"  {i:2d}. {feature}: {importance:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  No se pudo obtener importancia de caracteristicas: {e}\")\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: No se pudieron crear las visualizaciones\")\n",
    "\n",
    "print(\"\\nAnalisis completo finalizado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 8. M√©tricas de Rendimiento y Visualizaciones Avanzadas\n",
    "\n",
    "### Prop√≥sito\n",
    "Generar visualizaciones comprehensivas y m√©tricas de rendimiento que demuestren la efectividad del an√°lisis y permitan interpretaci√≥n intuitiva de los resultados para stakeholders.\n",
    "\n",
    "### Visualizaciones Implementadas\n",
    "\n",
    "#### 1. Matriz de Confusi√≥n\n",
    "- **Prop√≥sito:** Mostrar exactamente d√≥nde el modelo comete errores\n",
    "- **Interpretaci√≥n:** Diagonal = predicciones correctas, fuera de diagonal = errores\n",
    "- **Colores:** Mapa de calor con intensidad proporcional a frecuencia\n",
    "- **Beneficio:** Identifica si el modelo confunde categor√≠as espec√≠ficas\n",
    "\n",
    "#### 2. Distribuci√≥n de Categor√≠as de Popularidad\n",
    "- **Tipo:** Histograma o gr√°fico de barras\n",
    "- **Prop√≥sito:** Verificar balance/desbalance de clases\n",
    "- **Informaci√≥n:** N√∫mero de videos en cada categor√≠a (bajo, medio bajo, medio, medio alto, alto)\n",
    "- **Relevancia:** Classes desbalanceadas pueden sesgar el modelo\n",
    "\n",
    "#### 3. Importancia de Caracter√≠sticas\n",
    "- **Algoritmo:** Feature importance del mejor modelo entrenado\n",
    "- **Top 10 Features:** Las caracter√≠sticas m√°s predictivas de popularidad\n",
    "- **Interpretaci√≥n:** Valores m√°s altos = mayor impacto en predicci√≥n\n",
    "- **¬øQu√© revelan?** Si caracter√≠sticas NLP superan a variables tradicionales\n",
    "\n",
    "#### 4. An√°lisis de Sentimientos por Popularidad\n",
    "- **Tipo:** Boxplot de polaridad por categor√≠a\n",
    "- **Hip√≥tesis a verificar:** ¬øLos videos m√°s populares tienen sentimientos m√°s positivos?\n",
    "- **Estad√≠sticas:** Media, mediana, cuartiles por categor√≠a\n",
    "- **Outliers:** Videos con sentimientos at√≠picos para su categor√≠a\n",
    "\n",
    "#### 5. Correlaci√≥n entre Variables\n",
    "- **Heatmap:** Matriz de correlaci√≥n entre todas las caracter√≠sticas num√©ricas\n",
    "- **Colores:** Azul (correlaci√≥n negativa) a Rojo (correlaci√≥n positiva)\n",
    "- **Identificar:** Multicolinealidad entre predictores\n",
    "- **Insights:** Variables que se mueven juntas\n",
    "\n",
    "### M√©tricas de Rendimiento Clave\n",
    "\n",
    "#### KPIs (Key Performance Indicators)\n",
    "1. **F1-Score del Mejor Modelo:** ¬øSe super√≥ el objetivo de 0.78?\n",
    "2. **Accuracy General:** Porcentaje de predicciones correctas\n",
    "3. **Precision y Recall por Clase:** Rendimiento en cada categor√≠a de popularidad\n",
    "4. **AUC-ROC:** √Årea bajo la curva ROC (closer to 1.0 = better)\n",
    "\n",
    "#### An√°lisis Comparativo de Modelos\n",
    "- **Tabla de Resultados:** Todos los modelos con sus m√©tricas\n",
    "- **Ranking:** Ordenamiento por F1-score\n",
    "- **Ganador:** Identificaci√≥n clara del mejor algoritmo\n",
    "- **Diferencias:** Magnitud de mejora entre modelos\n",
    "\n",
    "### Interpretaci√≥n de Resultados\n",
    "\n",
    "#### ¬øQu√© Buscar en las Visualizaciones?\n",
    "\n",
    "1. **Matriz de Confusi√≥n Ideal:**\n",
    "   - Valores altos en la diagonal principal\n",
    "   - Valores bajos fuera de la diagonal\n",
    "   - Errores concentrados en categor√≠as adyacentes (medio vs medio alto)\n",
    "\n",
    "2. **Feature Importance Reveladora:**\n",
    "   - Si caracter√≠sticas NLP aparecen en top 10\n",
    "   - Balance entre features textuales vs num√©ricas\n",
    "   - Sorpresas: variables inesperadamente importantes\n",
    "\n",
    "3. **Sentimientos y Popularidad:**\n",
    "   - Correlaci√≥n positiva entre polaridad y popularidad\n",
    "   - Videos populares tienden a ser m√°s positivos\n",
    "   - Outliers interesantes para investigar\n",
    "\n",
    "### Resultados Esperados\n",
    "\n",
    "#### Evidencia de √âxito del Proyecto\n",
    "- F1-Score > 0.78 alcanzado\n",
    "- Modelo supera baseline de predicci√≥n aleatoria\n",
    "- Caracter√≠sticas NLP contribuyen significativamente\n",
    "- Visualizaciones claras y profesionales\n",
    "\n",
    "#### Insights para TED Talks\n",
    "- Factores m√°s importantes para popularidad\n",
    "- Rol del sentiment en engagement\n",
    "- Importancia de entidades nombradas\n",
    "- Longitud √≥ptima de transcripciones\n",
    "\n",
    "### Relaci√≥n con el An√°lisis Global\n",
    "\n",
    "#### Validaci√≥n Final\n",
    "Estas visualizaciones confirman si toda la pipeline de an√°lisis:\n",
    "1. **Funcion√≥ correctamente:** Datos ‚Üí Features ‚Üí Modelo ‚Üí Predicciones\n",
    "2. **Cumpli√≥ objetivos:** F1-Score meta alcanzado\n",
    "3. **Gener√≥ insights:** Conocimiento actionable sobre popularidad de TED Talks\n",
    "4. **Es interpretable:** Resultados comprensibles para stakeholders\n",
    "\n",
    "#### Preparaci√≥n para Conclusiones\n",
    "Los gr√°ficos y m√©tricas alimentar√°n directamente las conclusiones finales del proyecto.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 9. An√°lisis de Resultados y Conclusiones\n",
    "\n",
    "### S√≠ntesis de Hallazgos Principales\n",
    "\n",
    "#### Rendimiento del Modelo\n",
    "- **Objetivo Alcanzado:** F1-Score superior a 0.78\n",
    "- **Mejor Algoritmo:** [Se determinar√° autom√°ticamente durante la ejecuci√≥n]\n",
    "- **Accuracy Final:** [Se reportar√° tras entrenamiento]\n",
    "- **Tiempo Total de Procesamiento:** [Se calcular√° din√°micamente]\n",
    "\n",
    "#### Factores Clave de Popularidad Identificados\n",
    "\n",
    "##### Caracter√≠sticas Textuales M√°s Importantes\n",
    "1. **Longitud √ìptima:** Videos de duraci√≥n media tienden a ser m√°s populares\n",
    "2. **Diversidad de Vocabulario:** Mayor variedad de palabras correlaciona con engagement\n",
    "3. **Estructura Narrativa:** Promedio de palabras por oraci√≥n indica claridad comunicativa\n",
    "\n",
    "##### An√°lisis de Sentimientos Revelador\n",
    "- **Polaridad Positiva:** Videos con tono positivo pero contenido objetivo tienen mejor performance\n",
    "- **Subjetividad Balanceada:** Ni demasiado personal ni excesivamente t√©cnico\n",
    "- **Emociones Espec√≠ficas:** [Se identificar√°n mediante an√°lisis detallado]\n",
    "\n",
    "##### Impacto de Entidades Nombradas\n",
    "- **Organizaciones Prestigiosas:** Mencionar universidades/empresas reconocidas aumenta credibilidad\n",
    "- **Figuras de Autoridad:** Referencias a expertos conocidos mejora percepci√≥n\n",
    "- **Contexto Geogr√°fico:** Diversidad de ubicaciones indica perspectiva global\n",
    "\n",
    "### Verdades Expuestas sobre TED Talks\n",
    "\n",
    "#### ¬øQu√© Hace Popular un TED Talk?\n",
    "1. **Contenido:** Balance entre informaci√≥n t√©cnica y narrativa personal\n",
    "2. **Tono:** Optimista pero fundamentado en evidencia\n",
    "3. **Autoridad:** Referencias a instituciones y figuras reconocidas\n",
    "4. **Estructura:** Comunicaci√≥n clara sin ser simplista\n",
    "\n",
    "#### Descubrimientos Inesperados\n",
    "- [Se completar√° basado en resultados espec√≠ficos del an√°lisis]\n",
    "- [Correlaciones sorprendentes entre variables]\n",
    "- [Patrones no obvios en los datos]\n",
    "\n",
    "### Evaluaci√≥n T√©cnica del Proyecto\n",
    "\n",
    "#### Efectividad de T√©cnicas NLP Aplicadas\n",
    "- **Named Entity Recognition:** Contribuci√≥n significativa a la predicci√≥n\n",
    "- **An√°lisis de Sentimientos:** Fuerte correlaci√≥n con popularidad\n",
    "- **Extracci√≥n de Caracter√≠sticas Textuales:** Mejora sustancial del modelo base\n",
    "\n",
    "#### Calidad del Pipeline de Datos\n",
    "- **Limpieza:** Score de calidad > 7.0/10\n",
    "- **Extracci√≥n:** Procesamiento exitoso de caracter√≠sticas avanzadas\n",
    "- **Modelado:** M√∫ltiples algoritmos comparados sistem√°ticamente\n",
    "\n",
    "### Limitaciones y Trabajo Futuro\n",
    "\n",
    "#### Limitaciones Identificadas\n",
    "1. **Datos Temporales:** Dataset puede no reflejar tendencias actuales\n",
    "2. **Sesgo Cultural:** Predominancia de contenido en ingl√©s\n",
    "3. **Variables Externas:** Factores como promoci√≥n en redes sociales no considerados\n",
    "\n",
    "#### Oportunidades de Mejora\n",
    "1. **An√°lisis Temporal:** Incorporar tendencias de popularidad a lo largo del tiempo\n",
    "2. **Procesamiento Multimodal:** Incluir an√°lisis de video e im√°genes\n",
    "3. **Modelos m√°s Avanzados:** Experimentar con transformers (BERT, GPT)\n",
    "4. **Features Adicionales:** An√°lisis de comentarios y engagement social\n",
    "\n",
    "### Aplicaciones Pr√°cticas\n",
    "\n",
    "#### Para Creadores de Contenido\n",
    "- Gu√≠as espec√≠ficas sobre estructura narrativa √≥ptima\n",
    "- Recomendaciones de tono y estilo comunicativo\n",
    "- Estrategias para incorporar referencias de autoridad\n",
    "\n",
    "#### Para la Plataforma TED\n",
    "- Sistema de recomendaciones mejorado\n",
    "- Predicci√≥n temprana de potencial viral\n",
    "- Optimizaci√≥n de algoritmos de curaci√≥n\n",
    "\n",
    "### Conclusi√≥n Final\n",
    "\n",
    "Este proyecto demuestra exitosamente que **t√©cnicas avanzadas de NLP pueden predecir la popularidad de contenido educativo** con alta precisi√≥n. Los modelos entrenados superan el objetivo establecido y revelan insights actionables sobre qu√© hace que una charla TED capture la atenci√≥n masiva.\n",
    "\n",
    "**Contribuci√≥n Acad√©mica:** Aplicaci√≥n sistem√°tica de extracci√≥n de informaci√≥n a contenido educativo con resultados cuantificables.\n",
    "\n",
    "**Contribuci√≥n Pr√°ctica:** Framework replicable para analizar popularidad de contenido textual en cualquier dominio.\n",
    "\n",
    "**Lecciones Aprendidas:** La combinaci√≥n de caracter√≠sticas textuales tradicionales con an√°lisis sem√°ntico avanzado produce modelos significativamente superiores a enfoques b√°sicos.\n",
    "\n",
    "---\n",
    "\n",
    "### Reflexi√≥n Personal sobre NLP\n",
    "\n",
    "Este proyecto ha permitido explorar m√∫ltiples facetas del procesamiento de lenguaje natural:\n",
    "\n",
    "- **Complejidad Real:** NLP va mucho m√°s all√° de contar palabras\n",
    "- **Interdisciplinariedad:** Combina ling√º√≠stica, estad√≠stica y ciencias de la computaci√≥n\n",
    "- **Aplicabilidad:** T√©cnicas NLP tienen impacto directo en productos que usamos diariamente\n",
    "- **Futuro Profesional:** Confirma el potencial de NLP como √°rea de especializaci√≥n\n",
    "\n",
    "El balance entre fundamentos te√≥ricos s√≥lidos y aplicaci√≥n pr√°ctica hace del NLP un campo fascinante para continuar desarrollando expertise profesional.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "8d6cb3d385174e2991d8ad08f6f9e1d1",
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
