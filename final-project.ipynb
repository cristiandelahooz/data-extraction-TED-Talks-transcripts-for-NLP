{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "1fbc8c4a00154002a05a0c549d6ac802",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 19467,
    "execution_start": 1754285854636,
    "source_hash": "1066cd4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modulos cargados correctamente\n"
     ]
    }
   ],
   "source": [
    "# === ANALISIS DE POPULARIDAD DE TED TALKS ===\n",
    "# Aplicacion de Extraccion de Informacion y Comparacion de Modelos ML\n",
    "\n",
    "# Importar la clase principal que controla todo el flujo\n",
    "from modules import TedTalkAnalyzer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Modulos cargados correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "421855a6adaf4a6f90e73d9a35b69a05",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 1,
    "execution_start": 1754285874155,
    "source_hash": "65ab57b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando instancia del analizador TED Talks...\n",
      "Analizador creado correctamente\n",
      "Metodos disponibles:\n",
      "- setup_environment(): Configurar ambiente\n",
      "- load_data(): Cargar datos\n",
      "- clean_data(): Limpiar datos\n",
      "- process_nlp_features(): Procesar NLP\n",
      "- train_models(): Entrenar modelos ML\n",
      "- create_visualizations(): Crear graficos\n",
      "- run_complete_analysis(): Ejecutar todo automaticamente\n"
     ]
    }
   ],
   "source": [
    "# === CREAR INSTANCIA DEL ANALIZADOR ===\n",
    "\n",
    "print(\"Creando instancia del analizador TED Talks...\")\n",
    "\n",
    "# Crear instancia de la clase principal\n",
    "analyzer = TedTalkAnalyzer()\n",
    "\n",
    "print(\"Analizador creado correctamente\")\n",
    "print(\"Metodos disponibles:\")\n",
    "print(\"- setup_environment(): Configurar ambiente\")\n",
    "print(\"- load_data(): Cargar datos\")\n",
    "print(\"- clean_data(): Limpiar datos\")\n",
    "print(\"- process_nlp_features(): Procesar NLP\")\n",
    "print(\"- train_models(): Entrenar modelos ML\")\n",
    "print(\"- create_visualizations(): Crear graficos\")\n",
    "print(\"- run_complete_analysis(): Ejecutar todo automaticamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "c7095e88845b4fe19708d4df2669befb",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 50249,
    "execution_start": 1754285874215,
    "source_hash": "64b5d9ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INICIO: 11:35:26\n",
      "Configurando ambiente y dependencias...\n",
      "Esto puede tomar 2-5 minutos la primera vez\n",
      "==================================================\n",
      "\n",
      "=== CONFIGURANDO AMBIENTE ===\n",
      "=== CONFIGURACION DEL AMBIENTE ===\n",
      "Tiempo estimado: 2-5 minutos\n",
      "\n",
      "PASO 1/3: Instalando 8 paquetes esenciales...\n",
      "  [1/8] Instalando pandas>=1.3.0... OK\n",
      "  [2/8] Instalando numpy>=2.0.0... OK\n",
      "  [3/8] Instalando scikit-learn>=1.0.0... OK\n",
      "  [4/8] Instalando matplotlib>=3.4.0... OK\n",
      "  [5/8] Instalando seaborn>=0.11.0... OK\n",
      "  [6/8] Instalando nltk>=3.7... OK\n",
      "  [7/8] Instalando textblob>=0.17.0... OK\n",
      "  [8/8] Instalando tqdm>=4.64.0... OK\n",
      "\n",
      "Paquetes esenciales: 8/8 instalados\n",
      "\n",
      "PASO 2/3: Instalando 3 paquetes opcionales...\n",
      "  [1/3] Instalando plotly>=5.0.0... OK\n",
      "  [2/3] Instalando spacy>=3.4.0... OK\n",
      "  [3/3] Instalando wordcloud>=1.8.0... OK\n",
      "\n",
      "Paquetes opcionales: 3/3 instalados\n",
      "\n",
      "PASO 3/3: Configurando modelos de NLP...\n",
      "  Descargando datos NLTK...OK\n",
      "  Verificando spaCy..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OK\n",
      "\n",
      "CONFIGURACION COMPLETADA\n",
      "========================================\n",
      "✓ GPU disponible: Tesla T4\n",
      "✓ spaCy cargado correctamente\n",
      "✓ NLTK configurado correctamente\n",
      "Ambiente configurado correctamente\n",
      "\n",
      "Tiempo total: 38.7 segundos\n",
      "COMPLETADO: 11:36:04\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURACION DEL AMBIENTE ===\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"INICIO:\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "print(\"Configurando ambiente y dependencias...\")\n",
    "print(\"Esto puede tomar 2-5 minutos la primera vez\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configurar ambiente usando el metodo del analizador\n",
    "start_time = time.time()\n",
    "analyzer.setup_environment()\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed = end_time - start_time\n",
    "print(f\"\\nTiempo total: {elapsed:.1f} segundos\")\n",
    "print(f\"COMPLETADO:\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "ea1bbcae76a446a08d44ec5f68a6190f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 1125,
    "execution_start": 1754286173402,
    "source_hash": "f2aee897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset ted_talks_en.csv...\n",
      "\n",
      "=== CARGANDO DATASET: ted_talks_en.csv ===\n",
      "Dataset cargado: 4005 filas x 19 columnas\n",
      "\n",
      "Columnas disponibles:\n",
      " 1. talk_id\n",
      " 2. title\n",
      " 3. speaker_1\n",
      " 4. all_speakers\n",
      " 5. occupations\n",
      " 6. about_speakers\n",
      " 7. views\n",
      " 8. recorded_date\n",
      " 9. published_date\n",
      "10. event\n",
      "11. native_lang\n",
      "12. available_lang\n",
      "13. comments\n",
      "14. duration\n",
      "15. topics\n",
      "16. related_talks\n",
      "17. url\n",
      "18. description\n",
      "19. transcript\n",
      "Dataset cargado exitosamente\n",
      "Filas: 4,005\n",
      "Columnas: 19\n",
      "Memoria utilizada: 81.26 MB\n",
      "\n",
      "Columnas disponibles:\n",
      "  1. talk_id\n",
      "  2. title\n",
      "  3. speaker_1\n",
      "  4. all_speakers\n",
      "  5. occupations\n",
      "  6. about_speakers\n",
      "  7. views\n",
      "  8. recorded_date\n",
      "  9. published_date\n",
      "  10. event\n",
      "  11. native_lang\n",
      "  12. available_lang\n",
      "  13. comments\n",
      "  14. duration\n",
      "  15. topics\n",
      "  16. related_talks\n",
      "  17. url\n",
      "  18. description\n",
      "  19. transcript\n"
     ]
    }
   ],
   "source": [
    "# === CARGA DE DATOS ===\n",
    "\n",
    "print(\"Cargando dataset ted_talks_en.csv...\")\n",
    "\n",
    "# Cargar datos usando el metodo del analizador\n",
    "analyzer.load_data('ted_talks_en.csv')\n",
    "\n",
    "# Mostrar informacion basica\n",
    "if hasattr(analyzer, 'data') and analyzer.data is not None:\n",
    "    print(f\"Dataset cargado exitosamente\")\n",
    "    print(f\"Filas: {analyzer.data.shape[0]:,}\")\n",
    "    print(f\"Columnas: {analyzer.data.shape[1]}\")\n",
    "    print(f\"Memoria utilizada: {analyzer.data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Mostrar primeras columnas\n",
    "    print(\"\\nColumnas disponibles:\")\n",
    "    for i, col in enumerate(analyzer.data.columns):\n",
    "        print(f\"  {i+1}. {col}\")\n",
    "else:\n",
    "    print(\"ERROR: No se pudo cargar el dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "3502e50b80ca4a468e9a9a09f436fe09",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 4925,
    "execution_start": 1754286262535,
    "source_hash": "1b81bd79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando limpieza profesional de datos...\n",
      "\n",
      "=== LIMPIANDO DATOS ===\n",
      "Iniciando: Iniciando limpieza\n",
      "Tiempo de inicio: 11:36:06\n",
      "==================================================\n",
      "[11:36:06] Dataset original: 4005 filas x 19 columnas\n",
      "[1/4] (25.0%) Eliminando outliers con método IQR... OK\n",
      "Analizando distribución de 'views'...\n",
      "   - Q1 (25%): 882,069\n",
      "   - Q3 (75%): 2,133,110\n",
      "   - IQR: 1,251,041\n",
      "   - Límite inferior: -994,492\n",
      "   - Límite superior: 4,009,672\n",
      "   - Outliers identificados: 393 (9.81%)\n",
      "[11:36:07] Dataset después de eliminar outliers: 3612 filas\n",
      "[2/4] (50.0%) Limpiando datos textuales... OK\n",
      "[11:36:07] Procesando columna: title\n",
      " - Valores vacíos: 0\n",
      " - Longitud promedio: 38.4 caracteres\n",
      "[11:36:07] Procesando columna: description\n",
      " - Valores vacíos: 0\n",
      " - Longitud promedio: 352.8 caracteres\n",
      "[11:36:07] Procesando columna: transcript\n",
      " - Valores vacíos: 0\n",
      " - Longitud promedio: 9870.6 caracteres\n",
      "[11:36:13] Procesadas 3 columnas de texto\n",
      "[3/4] (75.0%) Creando categorías de popularidad... OK\n",
      " Analizando distribución de popularidad...\n",
      "   Umbrales de popularidad:\n",
      "     - Bajo: hasta 695,206 views\n",
      "     - Medio Bajo: hasta 1,111,279 views\n",
      "     - Medio: hasta 1,470,199 views\n",
      "     - Medio Alto: hasta 1,994,938 views\n",
      "     - Alto: hasta 4,006,448 views\n",
      "\n",
      "   Distribución de categorías:\n",
      "     - Bajo: 723 (20.0%)\n",
      "     - Medio Bajo: 722 (20.0%)\n",
      "     - Medio: 722 (20.0%)\n",
      "     - Medio Alto: 722 (20.0%)\n",
      "     - Alto: 723 (20.0%)\n",
      "[4/4] (100.0%) Validando dataset limpio... OK\n",
      "[11:36:13] Dimensiones finales: 3612 filas x 24 columnas\n",
      "[11:36:13] Filas eliminadas: 393 (9.81%)\n",
      "[11:36:13] Puntuación de calidad: 7.85/10\n",
      "\n",
      "==================================================\n",
      "Estado: Limpieza de datos completada\n",
      "Tiempo total: 6.6 segundos\n",
      "Finalizado: 11:36:13\n",
      "Tiempo promedio por paso: 1.7s\n",
      "Datos limpiados correctamente\n",
      "\n",
      "Resultados de la limpieza:\n",
      "  Filas originales: 4,005\n",
      "  Filas despues de limpieza: 3,612\n",
      "  Filas eliminadas: 393 (9.8%)\n",
      "\n",
      "Categorias de popularidad:\n",
      "  Bajo: 723 videos\n",
      "  Medio Bajo: 722 videos\n",
      "  Medio: 722 videos\n",
      "  Medio Alto: 722 videos\n",
      "  Alto: 723 videos\n"
     ]
    }
   ],
   "source": [
    "# === LIMPIEZA DE DATOS ===\n",
    "\n",
    "print(\"Aplicando limpieza profesional de datos...\")\n",
    "\n",
    "# Limpiar datos usando el metodo del analizador\n",
    "analyzer.clean_data()\n",
    "\n",
    "# Mostrar resultados de la limpieza\n",
    "if hasattr(analyzer, 'df_clean') and analyzer.df_clean is not None:\n",
    "    original_count = analyzer.data.shape[0]\n",
    "    clean_count = analyzer.df_clean.shape[0]\n",
    "    removed_count = original_count - clean_count\n",
    "    \n",
    "    print(f\"\\nResultados de la limpieza:\")\n",
    "    print(f\"  Filas originales: {original_count:,}\")\n",
    "    print(f\"  Filas despues de limpieza: {clean_count:,}\")\n",
    "    print(f\"  Filas eliminadas: {removed_count:,} ({removed_count/original_count*100:.1f}%)\")\n",
    "    \n",
    "    # Mostrar categorias de popularidad creadas\n",
    "    if 'popularity_category' in analyzer.df_clean.columns:\n",
    "        print(\"\\nCategorias de popularidad:\")\n",
    "        categories = analyzer.df_clean['popularity_category'].value_counts().sort_index()\n",
    "        for category, count in categories.items():\n",
    "            print(f\"  {category}: {count:,} videos\")\n",
    "            \n",
    "    # Mostrar calidad de datos\n",
    "    if 'df_cleaning' in analyzer.results:\n",
    "        quality_score = analyzer.results['df_cleaning']['quality_results']['quality_score']\n",
    "        print(f\"\\nPuntuacion de calidad de datos: {quality_score:.2f}/10\")\n",
    "else:\n",
    "    print(\"ERROR: No se pudo limpiar el dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a62b3e99d499449a8f1e5658cb1acfbb",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 22370,
    "execution_start": 1754286327855,
    "source_hash": "7f506687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando tecnicas de extraccion de informacion...\n",
      "Procesando: sentimientos, entidades nombradas, caracteristicas textuales\n",
      "\n",
      "=== PROCESANDO CARACTERÍSTICAS NLP ===\n",
      "Iniciando: Iniciando extracción de características NLP\n",
      "Tiempo de inicio: 11:36:13\n",
      "==================================================\n",
      "[11:36:13] Procesando columna: transcript_clean\n",
      "[1/5] (20.0%) Cargando modelos de NLP... OK\n",
      "[2/5] (40.0%) Preparando muestra de datos... OK\n",
      "[11:36:14] Procesando muestra de 100 textos para velocidad...\n",
      "[3/5] (60.0%) Analizando sentimientos con TextBlob... OK\n",
      "[11:36:14] Analizando polaridad y subjetividad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deca1d1eb1744fbd938460964bd4bd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentimientos:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:36:27] Análisis de sentimientos completado\n",
      "[4/5] (80.0%) Extrayendo características textuales... OK\n",
      "[11:36:27] Calculando longitud, palabras, oraciones...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a442854dac494f2aa7d04dd92f5f5757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Características:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:36:31] Características textuales extraídas\n",
      "[5/5] (100.0%) Identificando entidades nombradas... OK\n",
      "Procesando 100 textos para entidades nombradas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eedbf6c23a640c492fe9e9b70f3a7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entidades:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === EXTRACCION DE INFORMACION CON NLP ===\n",
    "\n",
    "print(\"Aplicando tecnicas de extraccion de informacion...\")\n",
    "print(\"Procesando: sentimientos, entidades nombradas, caracteristicas textuales\")\n",
    "\n",
    "# Procesar caracteristicas NLP usando el metodo del analizador\n",
    "analyzer.process_nlp_features(text_column='transcript_clean')\n",
    "\n",
    "# Mostrar caracteristicas extraidas\n",
    "# Fix: Check if 'df_processed' attribute exists before accessing it\n",
    "if hasattr(analyzer, 'df_processed') and analyzer.df_processed is not None:\n",
    "    print(f\"\\nExtraccion de informacion completada\")\n",
    "    print(f\"Dataset procesado: {analyzer.df_processed.shape}\")\n",
    "    \n",
    "    # Identificar caracteristicas NLP creadas\n",
    "    nlp_features = [col for col in analyzer.df_processed.columns if \n",
    "                   col.startswith(('sentiment_', 'text_', 'person_', 'org_', 'gpe_'))]\n",
    "    \n",
    "    print(f\"\\nCaracteristicas NLP extraidas: {len(nlp_features)}\")\n",
    "    print(\"Tipos de informacion extraida:\")\n",
    "    \n",
    "    # Agrupar por tipo\n",
    "    sentiment_features = [f for f in nlp_features if f.startswith('sentiment_')]\n",
    "    text_features = [f for f in nlp_features if f.startswith('text_')]\n",
    "    entity_features = [f for f in nlp_features if f.startswith(('person_', 'org_', 'gpe_'))]\n",
    "    \n",
    "    if sentiment_features:\n",
    "        print(f\"  Analisis de sentimientos: {len(sentiment_features)} caracteristicas\")\n",
    "    if text_features:\n",
    "        print(f\"  Caracteristicas textuales: {len(text_features)} caracteristicas\") \n",
    "    if entity_features:\n",
    "        print(f\"  Entidades nombradas: {len(entity_features)} caracteristicas\")\n",
    "        \n",
    "    # Mostrar estadisticas de muestra procesada\n",
    "    if 'nlp_processing' in analyzer.results:\n",
    "        word_frequencies = analyzer.results['nlp_processing']['word_frequencies']\n",
    "        print(f\"\\nMuestra procesada: {word_frequencies} registros\")\n",
    "else:\n",
    "    print(\"ERROR: No se pudo procesar las caracteristicas NLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:03:25.803244Z",
     "start_time": "2025-08-03T07:03:25.143615Z"
    },
    "cell_id": "8c9511cdd7ca47c9988212864a9ff788",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 38697,
    "execution_start": 1754286482018,
    "source_hash": "a6a134a7"
   },
   "outputs": [],
   "source": [
    "# === ENTRENAMIENTO Y COMPARACION DE MODELOS ML ===\n",
    "\n",
    "print(\"Entrenando y comparando modelos de Machine Learning...\")\n",
    "print(\"Objetivo: F1-score > 0.78\")\n",
    "\n",
    "# Entrenar modelos usando el metodo del analizador\n",
    "analyzer.train_models(text_column='transcript_clean', target_column='popularity_numeric')\n",
    "\n",
    "# Mostrar resultados de los modelos\n",
    "if 'machine_learning' in analyzer.results:\n",
    "    # Check if 'evaluation_results' key exists before accessing it\n",
    "    if 'evaluation_results' in analyzer.results['machine_learning']:\n",
    "        ml_results = analyzer.results['machine_learning']['evaluation_results']\n",
    "        models_trained = analyzer.results['machine_learning']['models_trained']\n",
    "        \n",
    "        print(\"\\nRESULTADOS DE MODELOS:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Mostrar resultados de cada modelo\n",
    "        for model_name, results in ml_results.items():\n",
    "            if results is not None:\n",
    "                print(f\"\\n{model_name}:\")\n",
    "                print(f\"  Accuracy:  {results['accuracy']:.4f}\")\n",
    "                print(f\"  Precision: {results['precision']:.4f}\")\n",
    "                print(f\"  Recall:    {results['recall']:.4f}\")\n",
    "                print(f\"  F1-Score:  {results['f1_score']:.4f}\")\n",
    "                \n",
    "                # Verificar si cumple objetivo\n",
    "                objetivo_cumplido = \"SI\" if results['f1_score'] > 0.78 else \"NO\"\n",
    "                print(f\"  Objetivo F1>0.78: {objetivo_cumplido}\")\n",
    "        \n",
    "        # Identificar mejor modelo\n",
    "        best_model_name, best_model, best_score = analyzer.results['machine_learning']['best_model']\n",
    "        print(f\"\\nMEJOR MODELO: {best_model_name}\")\n",
    "        print(f\"F1-Score: {best_score:.4f}\")\n",
    "        \n",
    "        if best_score > 0.78:\n",
    "            print(\"Objetivo cumplido! F1-Score > 0.78\")\n",
    "        else:\n",
    "            print(\"Objetivo no cumplido. Considerar mas datos o mejores caracteristicas.\")\n",
    "            \n",
    "        # Guardar el mejor modelo para referencia\n",
    "        analyzer.best_model = best_model_name\n",
    "        analyzer.best_f1_score = best_score\n",
    "    else:\n",
    "        print(\"ERROR: 'evaluation_results' no estÃ¡ disponible en los resultados de machine_learning\")\n",
    "else:\n",
    "    print(\"ERROR: No se pudieron entrenar los modelos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T20:23:05.092879Z",
     "start_time": "2025-08-02T20:23:05.090755Z"
    },
    "cell_id": "cc67a9ef556e4065b9f55820612dba5d",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 0,
    "execution_start": 1754286520785,
    "source_hash": "176a87db"
   },
   "outputs": [],
   "source": [
    "# === METRICAS DE RENDIMIENTO Y VISUALIZACIONES ===\n",
    "\n",
    "print(\"Generando metricas de rendimiento y visualizaciones...\")\n",
    "\n",
    "# Crear visualizaciones usando el metodo del analizador\n",
    "analyzer.create_visualizations()\n",
    "\n",
    "# Mostrar informacion sobre las visualizaciones creadas\n",
    "if 'visualizations' in analyzer.results:\n",
    "    print(\"\\nVisualizaciones creadas exitosamente:\")\n",
    "    \n",
    "    # Si hay un clasificador disponible, mostrar importancia de caracteristicas\n",
    "    if hasattr(analyzer, 'best_model') and 'machine_learning' in analyzer.results:\n",
    "        models_trained = analyzer.results['machine_learning']['models_trained']\n",
    "        \n",
    "        print(f\"\\nImportancia de caracteristicas del mejor modelo ({analyzer.best_model}):\")\n",
    "        try:\n",
    "            feature_importance = models_trained.get_feature_importance(analyzer.best_model, top_n=10)\n",
    "            for i, (feature, importance) in enumerate(feature_importance, 1):\n",
    "                print(f\"  {i:2d}. {feature}: {importance:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  No se pudo obtener importancia de caracteristicas: {e}\")\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: No se pudieron crear las visualizaciones\")\n",
    "\n",
    "print(\"\\nAnalisis completo finalizado\")"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "8d6cb3d385174e2991d8ad08f6f9e1d1",
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
