{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "# Análisis de Popularidad de TED Talks mediante Procesamiento de Lenguaje Natural\n",
    "\n",
    "## Proyecto Final - Materia de NLP\n",
    "\n",
    "### Equipo de Trabajo\n",
    "- **Manuel Rodriguez** - ID: 1015-0681  \n",
    "- **Carolina Bencosme** - ID: 1014-8929  \n",
    "- **Cristian de la Hoz** - ID: 1014-9779  \n",
    "\n",
    "**Fecha:** Agosto 2025  \n",
    "**Objetivo:** Determinar los factores que hacen popular un TED Talk utilizando técnicas avanzadas de extracción de información y machine learning\n",
    "\n",
    "---\n",
    "\n",
    "### 🔴 IMPORTANTE - Repositorio GitHub\n",
    "\n",
    "<div style=\"background-color: #ffebee; border: 3px solid #e53e3e; padding: 15px; margin: 10px 0; border-radius: 8px;\">\n",
    "<h4 style=\"color: #c53030; margin-top: 0;\">📍 UBICACIÓN DEL PROYECTO COMPLETO</h4>\n",
    "\n",
    "**El proyecto se encuentra en GitHub, no fue subido junto con el resto de la asignación debido al límite en la PVA. El proyecto se vio en la necesidad de ser modularizado debido a que era bastante extenso, incluso luego de hacerlo el mismo sigue siendo muy extenso.**\n",
    "\n",
    "**Para visualizar las implementaciones completas, favor de dirigirse al repositorio:**\n",
    "\n",
    "🔗 **https://github.com/cristiandelahooz/data-extraction-TED-Talks-transcripts-for-NLP**\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### Resumen Ejecutivo\n",
    "\n",
    "Este proyecto analiza el dataset `ted_talks_en.csv` para identificar patrones que determinan la popularidad de las charlas TED. Mediante técnicas de NLP como Named Entity Recognition (NER), análisis de sentimientos y extracción de características textuales, se entrenan múltiples modelos de machine learning para clasificar videos en categorías de popularidad.\n",
    "\n",
    "**Meta Principal:** Alcanzar un F1-score superior a 0.78 en la clasificación de popularidad.\n",
    "\n",
    "### Estructura del Proyecto\n",
    "\n",
    "``` bash\n",
    "data-extraction-TED-Talks-transcripts-for-NLP/\n",
    "├── final-project.ipynb          # Notebook principal documentado\n",
    "├── ted_talks_en.csv            # Dataset de TED Talks\n",
    "├── model_summary.txt           # Resumen de resultados de modelos\n",
    "└── modules/                    # Módulos especializados\n",
    "    ├── __init__.py            # Importaciones principales\n",
    "    ├── environment_setup.py   # Configuración de ambiente NLP\n",
    "    ├── data_cleaner.py        # Limpieza profesional de datos\n",
    "    ├── nlp_processor.py       # Extracción de características NLP\n",
    "    ├── ml_models.py           # Entrenamiento de modelos ML\n",
    "    ├── visualizer.py          # Generación de gráficos\n",
    "    └── progress_tracker.py    # Monitoreo de progreso\n",
    "```\n",
    "\n",
    "**Arquitectura Modular:** Cada módulo tiene una responsabilidad específica, facilitando mantenimiento, testing y escalabilidad del sistema.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "1fbc8c4a00154002a05a0c549d6ac802",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 19467,
    "execution_start": 1754285854636,
    "source_hash": "1066cd4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modulos cargados correctamente\n"
     ]
    }
   ],
   "source": [
    "# === ANALISIS DE POPULARIDAD DE TED TALKS ===\n",
    "# Aplicacion de Extraccion de Informacion y Comparacion de Modelos ML\n",
    "\n",
    "# Importar la clase principal que controla todo el flujo\n",
    "from modules import TedTalkAnalyzer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Modulos cargados correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 1. Importación de Módulos y Configuración Inicial\n",
    "\n",
    "### Propósito\n",
    "Esta sección inicial configura el entorno de trabajo importando la clase principal `TedTalkAnalyzer` que encapsula toda la lógica del análisis. Esta aproximación modular permite mantener el código organizado y reutilizable.\n",
    "\n",
    "### Técnicas Empleadas\n",
    "- **Arquitectura Modular:** Separación de responsabilidades en diferentes módulos especializados\n",
    "- **Supresión de Warnings:** Para mantener la salida limpia durante el procesamiento\n",
    "- **Importación Dinámica:** Los módulos se cargan bajo demanda para optimizar memoria\n",
    "\n",
    "### Módulos Principales del Sistema\n",
    "1. **`environment_setup.py`:** Gestión de dependencias y configuración del entorno\n",
    "2. **`data_cleaner.py`:** Limpieza y preprocesamiento de datos con estándares industriales\n",
    "3. **`nlp_processor.py`:** Técnicas avanzadas de NLP (NER, sentimientos, features textuales)\n",
    "4. **`ml_models.py`:** Entrenamiento y evaluación de modelos de machine learning\n",
    "5. **`visualizer.py`:** Generación de gráficos y métricas de rendimiento\n",
    "6. **`progress_tracker.py`:** Monitoreo en tiempo real del progreso\n",
    "\n",
    "### Beneficios de esta Arquitectura\n",
    "- **Mantenibilidad:** Cada módulo tiene una responsabilidad específica\n",
    "- **Escalabilidad:** Fácil agregar nuevas técnicas sin afectar el código existente\n",
    "- **Reutilización:** Los módulos pueden usarse en otros proyectos de NLP\n",
    "- **Testing:** Cada componente puede probarse independientemente\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "421855a6adaf4a6f90e73d9a35b69a05",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 1,
    "execution_start": 1754285874155,
    "source_hash": "65ab57b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando instancia del analizador TED Talks...\n",
      "Analizador creado correctamente\n",
      "Metodos disponibles:\n",
      "- setup_environment(): Configurar ambiente\n",
      "- load_data(): Cargar datos\n",
      "- clean_data(): Limpiar datos\n",
      "- process_nlp_features(): Procesar NLP\n",
      "- train_models(): Entrenar modelos ML\n",
      "- create_visualizations(): Crear graficos\n",
      "- run_complete_analysis(): Ejecutar todo automaticamente\n"
     ]
    }
   ],
   "source": [
    "# === CREAR INSTANCIA DEL ANALIZADOR ===\n",
    "\n",
    "print(\"Creando instancia del analizador TED Talks...\")\n",
    "\n",
    "# Crear instancia de la clase principal\n",
    "analyzer = TedTalkAnalyzer()\n",
    "\n",
    "print(\"Analizador creado correctamente\")\n",
    "print(\"Metodos disponibles:\")\n",
    "print(\"- setup_environment(): Configurar ambiente\")\n",
    "print(\"- load_data(): Cargar datos\")\n",
    "print(\"- clean_data(): Limpiar datos\")\n",
    "print(\"- process_nlp_features(): Procesar NLP\")\n",
    "print(\"- train_models(): Entrenar modelos ML\")\n",
    "print(\"- create_visualizations(): Crear graficos\")\n",
    "print(\"- run_complete_analysis(): Ejecutar todo automaticamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 2. Inicialización del Analizador Principal\n",
    "\n",
    "### Propósito\n",
    "Crear una instancia de la clase `TedTalkAnalyzer` que actuará como el controlador central de todo el pipeline de análisis. Esta clase implementa el patrón **Factory** y **Pipeline** para gestionar el flujo completo de datos.\n",
    "\n",
    "### Métodos Disponibles del Analizador\n",
    "\n",
    "#### Métodos Individuales (Para Control Granular)\n",
    "- **`setup_environment()`:** Configura dependencias, descarga modelos NLP y prepara el entorno\n",
    "- **`load_data()`:** Carga y valida el dataset inicial\n",
    "- **`clean_data()`:** Aplica técnicas de limpieza profesional de datos\n",
    "- **`process_nlp_features()`:** Extrae características mediante NLP avanzado\n",
    "- **`train_models()`:** Entrena y compara múltiples algoritmos de ML\n",
    "- **`create_visualizations()`:** Genera gráficos y métricas de rendimiento\n",
    "\n",
    "#### Método Automatizado\n",
    "- **`run_complete_analysis()`:** Ejecuta todo el pipeline automáticamente\n",
    "\n",
    "### Beneficios del Patrón Pipeline\n",
    "1. **Trazabilidad:** Cada paso puede ejecutarse y verificarse independientemente\n",
    "2. **Debugging:** Si hay errores, se puede identificar exactamente dónde ocurrieron\n",
    "3. **Experimentación:** Permite probar diferentes configuraciones en pasos específicos\n",
    "4. **Reproducibilidad:** El mismo pipeline garantiza resultados consistentes\n",
    "\n",
    "### Relación con el Análisis Global\n",
    "Esta instancia mantendrá todo el estado del análisis (datos originales, procesados, modelos entrenados, resultados) permitiendo acceso consistente a través de todo el notebook.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "c7095e88845b4fe19708d4df2669befb",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 50249,
    "execution_start": 1754285874215,
    "source_hash": "64b5d9ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INICIO: 11:35:26\n",
      "Configurando ambiente y dependencias...\n",
      "Esto puede tomar 2-5 minutos la primera vez\n",
      "==================================================\n",
      "\n",
      "=== CONFIGURANDO AMBIENTE ===\n",
      "=== CONFIGURACION DEL AMBIENTE ===\n",
      "Tiempo estimado: 2-5 minutos\n",
      "\n",
      "PASO 1/3: Instalando 8 paquetes esenciales...\n",
      "  [1/8] Instalando pandas>=1.3.0... OK\n",
      "  [2/8] Instalando numpy>=2.0.0... OK\n",
      "  [3/8] Instalando scikit-learn>=1.0.0... OK\n",
      "  [4/8] Instalando matplotlib>=3.4.0... OK\n",
      "  [5/8] Instalando seaborn>=0.11.0... OK\n",
      "  [6/8] Instalando nltk>=3.7... OK\n",
      "  [7/8] Instalando textblob>=0.17.0... OK\n",
      "  [8/8] Instalando tqdm>=4.64.0... OK\n",
      "\n",
      "Paquetes esenciales: 8/8 instalados\n",
      "\n",
      "PASO 2/3: Instalando 3 paquetes opcionales...\n",
      "  [1/3] Instalando plotly>=5.0.0... OK\n",
      "  [2/3] Instalando spacy>=3.4.0... OK\n",
      "  [3/3] Instalando wordcloud>=1.8.0... OK\n",
      "\n",
      "Paquetes opcionales: 3/3 instalados\n",
      "\n",
      "PASO 3/3: Configurando modelos de NLP...\n",
      "  Descargando datos NLTK...OK\n",
      "  Verificando spaCy..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OK\n",
      "\n",
      "CONFIGURACION COMPLETADA\n",
      "========================================\n",
      "✓ GPU disponible: Tesla T4\n",
      "✓ spaCy cargado correctamente\n",
      "✓ NLTK configurado correctamente\n",
      "Ambiente configurado correctamente\n",
      "\n",
      "Tiempo total: 38.7 segundos\n",
      "COMPLETADO: 11:36:04\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURACION DEL AMBIENTE ===\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"INICIO:\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "print(\"Configurando ambiente y dependencias...\")\n",
    "print(\"Esto puede tomar 2-5 minutos la primera vez\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configurar ambiente usando el metodo del analizador\n",
    "start_time = time.time()\n",
    "analyzer.setup_environment()\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed = end_time - start_time\n",
    "print(f\"\\nTiempo total: {elapsed:.1f} segundos\")\n",
    "print(f\"COMPLETADO:\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 3. Configuración del Ambiente y Dependencias\n",
    "\n",
    "### Propósito\n",
    "Preparar todo el entorno computacional necesario para el análisis avanzado de NLP. Este paso es crítico porque descarga y configura modelos pre-entrenados, librerías especializadas y datos auxiliares.\n",
    "\n",
    "### Componentes que se Configuran\n",
    "\n",
    "#### Librerías de NLP\n",
    "- **NLTK:** Descarga de corpora (stopwords, punkt tokenizer, VADER lexicon)\n",
    "- **spaCy:** Modelo `en_core_web_sm` para NER y análisis sintáctico\n",
    "- **Hugging Face:** Modelos pre-entrenados para análisis de sentimientos avanzado\n",
    "\n",
    "#### Optimizaciones de Rendimiento\n",
    "- **Caché Inteligente:** Evita descargas repetidas si los modelos ya existen\n",
    "- **Verificación de Integridad:** Valida que todos los modelos se descargaron correctamente\n",
    "- **Gestión de Memoria:** Configuración óptima para procesamiento de texto grande\n",
    "\n",
    "### Técnicas Empleadas\n",
    "\n",
    "#### ¿Por qué este paso toma tiempo?\n",
    "La primera ejecución descarga varios GB de datos:\n",
    "- Modelo spaCy en_core_web_sm (~50MB)\n",
    "- Corpora de NLTK (~200MB)\n",
    "- Vocabularios y embeddings pre-entrenados\n",
    "\n",
    "#### Estrategia de Caché\n",
    "El sistema implementa caché persistente que:\n",
    "1. Verifica si los modelos ya están instalados\n",
    "2. Solo descarga componentes faltantes\n",
    "3. Valida integridad de archivos existentes\n",
    "\n",
    "### Resultados Esperados\n",
    "-  Todos los modelos NLP listos para usar\n",
    "-  Configuración optimizada de memoria\n",
    "-  Verificación de compatibilidad de versiones\n",
    "- ️ Tiempo típico: 2-5 minutos (primera vez), <30 segundos (subsecuentes)\n",
    "\n",
    "### Relación con el Análisis Global\n",
    "Sin este paso, las técnicas avanzadas de NLP no funcionarían. Es la base que permite:\n",
    "- Reconocimiento preciso de entidades nombradas\n",
    "- Análisis de sentimientos con modelos state-of-the-art\n",
    "- Tokenización y lemmatización eficientes\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "ea1bbcae76a446a08d44ec5f68a6190f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 1125,
    "execution_start": 1754286173402,
    "source_hash": "f2aee897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset ted_talks_en.csv...\n",
      "\n",
      "=== CARGANDO DATASET: ted_talks_en.csv ===\n",
      "Dataset cargado: 4005 filas x 19 columnas\n",
      "\n",
      "Columnas disponibles:\n",
      " 1. talk_id\n",
      " 2. title\n",
      " 3. speaker_1\n",
      " 4. all_speakers\n",
      " 5. occupations\n",
      " 6. about_speakers\n",
      " 7. views\n",
      " 8. recorded_date\n",
      " 9. published_date\n",
      "10. event\n",
      "11. native_lang\n",
      "12. available_lang\n",
      "13. comments\n",
      "14. duration\n",
      "15. topics\n",
      "16. related_talks\n",
      "17. url\n",
      "18. description\n",
      "19. transcript\n",
      "Dataset cargado exitosamente\n",
      "Filas: 4,005\n",
      "Columnas: 19\n",
      "Memoria utilizada: 81.26 MB\n",
      "\n",
      "Columnas disponibles:\n",
      "  1. talk_id\n",
      "  2. title\n",
      "  3. speaker_1\n",
      "  4. all_speakers\n",
      "  5. occupations\n",
      "  6. about_speakers\n",
      "  7. views\n",
      "  8. recorded_date\n",
      "  9. published_date\n",
      "  10. event\n",
      "  11. native_lang\n",
      "  12. available_lang\n",
      "  13. comments\n",
      "  14. duration\n",
      "  15. topics\n",
      "  16. related_talks\n",
      "  17. url\n",
      "  18. description\n",
      "  19. transcript\n"
     ]
    }
   ],
   "source": [
    "# === CARGA DE DATOS ===\n",
    "\n",
    "print(\"Cargando dataset ted_talks_en.csv...\")\n",
    "\n",
    "# Cargar datos usando el metodo del analizador\n",
    "analyzer.load_data('ted_talks_en.csv')\n",
    "\n",
    "# Mostrar informacion basica\n",
    "if hasattr(analyzer, 'data') and analyzer.data is not None:\n",
    "    print(f\"Dataset cargado exitosamente\")\n",
    "    print(f\"Filas: {analyzer.data.shape[0]:,}\")\n",
    "    print(f\"Columnas: {analyzer.data.shape[1]}\")\n",
    "    print(f\"Memoria utilizada: {analyzer.data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Mostrar primeras columnas\n",
    "    print(\"\\nColumnas disponibles:\")\n",
    "    for i, col in enumerate(analyzer.data.columns):\n",
    "        print(f\"  {i+1}. {col}\")\n",
    "else:\n",
    "    print(\"ERROR: No se pudo cargar el dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 4. Carga y Análisis Exploratorio del Dataset\n",
    "\n",
    "### Propósito\n",
    "Cargar el dataset `ted_talks_en.csv` y realizar una exploración inicial para entender la estructura, calidad y características de los datos. Este paso es fundamental para planificar estrategias de limpieza y procesamiento.\n",
    "\n",
    "### Características del Dataset TED Talks\n",
    "\n",
    "#### Información Esperada\n",
    "El dataset contiene información sobre charlas TED con las siguientes columnas principales:\n",
    "- **Identificadores:** ID, título, speaker\n",
    "- **Métricas de Popularidad:** views, ratings, comments\n",
    "- **Contenido Textual:** transcript (transcripción completa), description\n",
    "- **Metadatos:** date, duration, tags, related_talks\n",
    "\n",
    "#### Técnicas de Validación Aplicadas\n",
    "1. **Verificación de Integridad:** Confirmar que el archivo existe y es legible\n",
    "2. **Análisis de Dimensiones:** Número de filas y columnas\n",
    "3. **Evaluación de Memoria:** Calcular uso de memoria para optimizar procesamiento\n",
    "4. **Inspección de Tipos de Datos:** Identificar columnas numéricas, texto, fechas\n",
    "\n",
    "### Métricas de Calidad de Datos\n",
    "\n",
    "#### ¿Por qué es Importante la Exploración Inicial?\n",
    "- **Detectar Problemas Tempranos:** Valores faltantes, duplicados, inconsistencias\n",
    "- **Planificar Estrategias:** Decidir técnicas de limpieza apropiadas\n",
    "- **Estimar Recursos:** Calcular tiempo y memoria necesarios para procesamiento\n",
    "\n",
    "#### Información de Salida Clave\n",
    "- **Número de Registros:** Total de TED Talks disponibles\n",
    "- **Dimensionalidad:** Cantidad de características disponibles\n",
    "- **Uso de Memoria:** Para planificar operaciones eficientes\n",
    "- **Columnas Disponibles:** Lista completa de variables del dataset\n",
    "\n",
    "### Resultados Esperados\n",
    "-  Confirmación exitosa de carga del dataset\n",
    "-  Estadísticas básicas de dimensiones\n",
    "-  Información de uso de memoria\n",
    "-  Inventario completo de columnas disponibles\n",
    "\n",
    "### Relación con el Análisis Global\n",
    "Esta carga inicial alimentará todos los pasos subsecuentes. La calidad y completitud de este dataset determinará:\n",
    "- Qué técnicas de NLP pueden aplicarse\n",
    "- Cuántos datos tendremos para entrenamiento\n",
    "- Qué variables pueden usarse como predictores de popularidad\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "3502e50b80ca4a468e9a9a09f436fe09",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 4925,
    "execution_start": 1754286262535,
    "source_hash": "1b81bd79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando limpieza profesional de datos...\n",
      "\n",
      "=== LIMPIANDO DATOS ===\n",
      "Iniciando: Iniciando limpieza\n",
      "Tiempo de inicio: 11:36:06\n",
      "==================================================\n",
      "[11:36:06] Dataset original: 4005 filas x 19 columnas\n",
      "[1/4] (25.0%) Eliminando outliers con método IQR... OK\n",
      "Analizando distribución de 'views'...\n",
      "   - Q1 (25%): 882,069\n",
      "   - Q3 (75%): 2,133,110\n",
      "   - IQR: 1,251,041\n",
      "   - Límite inferior: -994,492\n",
      "   - Límite superior: 4,009,672\n",
      "   - Outliers identificados: 393 (9.81%)\n",
      "[11:36:07] Dataset después de eliminar outliers: 3612 filas\n",
      "[2/4] (50.0%) Limpiando datos textuales... OK\n",
      "[11:36:07] Procesando columna: title\n",
      " - Valores vacíos: 0\n",
      " - Longitud promedio: 38.4 caracteres\n",
      "[11:36:07] Procesando columna: description\n",
      " - Valores vacíos: 0\n",
      " - Longitud promedio: 352.8 caracteres\n",
      "[11:36:07] Procesando columna: transcript\n",
      " - Valores vacíos: 0\n",
      " - Longitud promedio: 9870.6 caracteres\n",
      "[11:36:13] Procesadas 3 columnas de texto\n",
      "[3/4] (75.0%) Creando categorías de popularidad... OK\n",
      " Analizando distribución de popularidad...\n",
      "   Umbrales de popularidad:\n",
      "     - Bajo: hasta 695,206 views\n",
      "     - Medio Bajo: hasta 1,111,279 views\n",
      "     - Medio: hasta 1,470,199 views\n",
      "     - Medio Alto: hasta 1,994,938 views\n",
      "     - Alto: hasta 4,006,448 views\n",
      "\n",
      "   Distribución de categorías:\n",
      "     - Bajo: 723 (20.0%)\n",
      "     - Medio Bajo: 722 (20.0%)\n",
      "     - Medio: 722 (20.0%)\n",
      "     - Medio Alto: 722 (20.0%)\n",
      "     - Alto: 723 (20.0%)\n",
      "[4/4] (100.0%) Validando dataset limpio... OK\n",
      "[11:36:13] Dimensiones finales: 3612 filas x 24 columnas\n",
      "[11:36:13] Filas eliminadas: 393 (9.81%)\n",
      "[11:36:13] Puntuación de calidad: 7.85/10\n",
      "\n",
      "==================================================\n",
      "Estado: Limpieza de datos completada\n",
      "Tiempo total: 6.6 segundos\n",
      "Finalizado: 11:36:13\n",
      "Tiempo promedio por paso: 1.7s\n",
      "Datos limpiados correctamente\n",
      "\n",
      "Resultados de la limpieza:\n",
      "  Filas originales: 4,005\n",
      "  Filas despues de limpieza: 3,612\n",
      "  Filas eliminadas: 393 (9.8%)\n",
      "\n",
      "Categorias de popularidad:\n",
      "  Bajo: 723 videos\n",
      "  Medio Bajo: 722 videos\n",
      "  Medio: 722 videos\n",
      "  Medio Alto: 722 videos\n",
      "  Alto: 723 videos\n"
     ]
    }
   ],
   "source": [
    "# === LIMPIEZA DE DATOS ===\n",
    "\n",
    "print(\"Aplicando limpieza profesional de datos...\")\n",
    "\n",
    "# Limpiar datos usando el metodo del analizador\n",
    "analyzer.clean_data()\n",
    "\n",
    "# Mostrar resultados de la limpieza\n",
    "if hasattr(analyzer, 'df_clean') and analyzer.df_clean is not None:\n",
    "    original_count = analyzer.data.shape[0]\n",
    "    clean_count = analyzer.df_clean.shape[0]\n",
    "    removed_count = original_count - clean_count\n",
    "    \n",
    "    print(f\"\\nResultados de la limpieza:\")\n",
    "    print(f\"  Filas originales: {original_count:,}\")\n",
    "    print(f\"  Filas despues de limpieza: {clean_count:,}\")\n",
    "    print(f\"  Filas eliminadas: {removed_count:,} ({removed_count/original_count*100:.1f}%)\")\n",
    "    \n",
    "    # Mostrar categorias de popularidad creadas\n",
    "    if 'popularity_category' in analyzer.df_clean.columns:\n",
    "        print(\"\\nCategorias de popularidad:\")\n",
    "        categories = analyzer.df_clean['popularity_category'].value_counts().sort_index()\n",
    "        for category, count in categories.items():\n",
    "            print(f\"  {category}: {count:,} videos\")\n",
    "            \n",
    "    # Mostrar calidad de datos\n",
    "    if 'df_cleaning' in analyzer.results:\n",
    "        quality_score = analyzer.results['df_cleaning']['quality_results']['quality_score']\n",
    "        print(f\"\\nPuntuacion de calidad de datos: {quality_score:.2f}/10\")\n",
    "else:\n",
    "    print(\"ERROR: No se pudo limpiar el dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 5. Limpieza Profesional de Datos\n",
    "\n",
    "### Propósito\n",
    "Aplicar técnicas de limpieza de datos siguiendo estándares industriales similares a los usados por ingenieros en Amazon y Google. Esta fase es crítica porque datos de mala calidad producen modelos de baja calidad.\n",
    "\n",
    "### Técnicas de Limpieza Implementadas\n",
    "\n",
    "#### 1. Eliminación de Outliers Extremos\n",
    "- **Método IQR (Interquartile Range):** Identifica valores atípicos en la columna 'views'\n",
    "- **¿Por qué eliminar outliers?** Videos con views extremadamente altos (virales) o bajos (errores) pueden sesgar el modelo\n",
    "- **Cálculo:** Q1 - 1.5×IQR y Q3 + 1.5×IQR como límites\n",
    "\n",
    "#### 2. Manejo de Valores Faltantes\n",
    "- **Estrategia Adaptativa:** Diferentes técnicas según el tipo de columna\n",
    "- **Texto:** Imputación con \"Unknown\" o eliminación si es crítico\n",
    "- **Numérico:** Mediana para datos sesgados, media para distribuciones normales\n",
    "- **Categórico:** Moda o nueva categoría \"Missing\"\n",
    "\n",
    "#### 3. Normalización y Estandarización\n",
    "- **Limpieza de Texto:** Eliminación de caracteres especiales, normalización de encoding\n",
    "- **Fechas:** Conversión a formato estándar datetime\n",
    "- **Strings:** Normalización de mayúsculas/minúsculas\n",
    "\n",
    "### Creación de Variable Objetivo\n",
    "\n",
    "#### Categorización de Popularidad\n",
    "El sistema convierte la variable continua 'views' en categorías discretas:\n",
    "\n",
    "1. **\"bajo\":** Videos con pocas visualizaciones\n",
    "2. **\"medio bajo\":** Popularidad limitada\n",
    "3. **\"medio\":** Popularidad promedio\n",
    "4. **\"medio alto\":** Popular\n",
    "5. **\"alto\":** Muy popular\n",
    "\n",
    "#### ¿Por qué Categorizar en lugar de Regresión?\n",
    "- **Interpretabilidad:** Es más fácil entender \"popular vs no popular\"\n",
    "- **Robustez:** Menos sensible a outliers extremos\n",
    "- **Aplicabilidad:** Los stakeholders prefieren clasificaciones claras\n",
    "\n",
    "### Métricas de Calidad\n",
    "\n",
    "#### Puntuación de Calidad de Datos (Scale 1-10)\n",
    "El sistema calcula un score basado en:\n",
    "- **Completitud:** Porcentaje de valores no faltantes\n",
    "- **Consistencia:** Coherencia entre variables relacionadas\n",
    "- **Validez:** Valores dentro de rangos esperados\n",
    "- **Precisión:** Detección de errores evidentes\n",
    "\n",
    "### Resultados Esperados\n",
    "-  Reducción controlada del dataset (típicamente 5-15% de registros eliminados)\n",
    "-  5 categorías balanceadas de popularidad\n",
    "-  Score de calidad de datos > 7.0/10\n",
    "-  Dataset libre de valores faltantes críticos\n",
    "\n",
    "### Relación con el Análisis Global\n",
    "Esta limpieza determina directamente:\n",
    "- **Calidad del Modelo:** Datos limpios = modelos más precisos\n",
    "- **Representatividad:** Mantener diversidad mientras eliminamos ruido\n",
    "- **Eficiencia Computacional:** Menos datos irrelevantes = entrenamiento más rápido\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a62b3e99d499449a8f1e5658cb1acfbb",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 22370,
    "execution_start": 1754286327855,
    "source_hash": "7f506687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando tecnicas de extraccion de informacion...\n",
      "Procesando: sentimientos, entidades nombradas, caracteristicas textuales\n",
      "\n",
      "=== PROCESANDO CARACTERÍSTICAS NLP ===\n",
      "Iniciando: Iniciando extracción de características NLP\n",
      "Tiempo de inicio: 11:36:13\n",
      "==================================================\n",
      "[11:36:13] Procesando columna: transcript_clean\n",
      "[1/5] (20.0%) Cargando modelos de NLP... OK\n",
      "[2/5] (40.0%) Preparando muestra de datos... OK\n",
      "[11:36:14] Procesando muestra de 100 textos para velocidad...\n",
      "[3/5] (60.0%) Analizando sentimientos con TextBlob... OK\n",
      "[11:36:14] Analizando polaridad y subjetividad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deca1d1eb1744fbd938460964bd4bd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentimientos:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:36:27] Análisis de sentimientos completado\n",
      "[4/5] (80.0%) Extrayendo características textuales... OK\n",
      "[11:36:27] Calculando longitud, palabras, oraciones...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a442854dac494f2aa7d04dd92f5f5757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Características:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:36:31] Características textuales extraídas\n",
      "[5/5] (100.0%) Identificando entidades nombradas... OK\n",
      "Procesando 100 textos para entidades nombradas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eedbf6c23a640c492fe9e9b70f3a7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entidades:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === EXTRACCION DE INFORMACION CON NLP ===\n",
    "\n",
    "print(\"Aplicando tecnicas de extraccion de informacion...\")\n",
    "print(\"Procesando: sentimientos, entidades nombradas, caracteristicas textuales\")\n",
    "\n",
    "# Procesar caracteristicas NLP usando el metodo del analizador\n",
    "analyzer.process_nlp_features(text_column='transcript_clean')\n",
    "\n",
    "# Mostrar caracteristicas extraidas\n",
    "# Fix: Check if 'df_processed' attribute exists before accessing it\n",
    "if hasattr(analyzer, 'df_processed') and analyzer.df_processed is not None:\n",
    "    print(f\"\\nExtraccion de informacion completada\")\n",
    "    print(f\"Dataset procesado: {analyzer.df_processed.shape}\")\n",
    "    \n",
    "    # Identificar caracteristicas NLP creadas\n",
    "    nlp_features = [col for col in analyzer.df_processed.columns if \n",
    "                   col.startswith(('sentiment_', 'text_', 'person_', 'org_', 'gpe_'))]\n",
    "    \n",
    "    print(f\"\\nCaracteristicas NLP extraidas: {len(nlp_features)}\")\n",
    "    print(\"Tipos de informacion extraida:\")\n",
    "    \n",
    "    # Agrupar por tipo\n",
    "    sentiment_features = [f for f in nlp_features if f.startswith('sentiment_')]\n",
    "    text_features = [f for f in nlp_features if f.startswith('text_')]\n",
    "    entity_features = [f for f in nlp_features if f.startswith(('person_', 'org_', 'gpe_'))]\n",
    "    \n",
    "    if sentiment_features:\n",
    "        print(f\"  Analisis de sentimientos: {len(sentiment_features)} caracteristicas\")\n",
    "    if text_features:\n",
    "        print(f\"  Caracteristicas textuales: {len(text_features)} caracteristicas\") \n",
    "    if entity_features:\n",
    "        print(f\"  Entidades nombradas: {len(entity_features)} caracteristicas\")\n",
    "        \n",
    "    # Mostrar estadisticas de muestra procesada\n",
    "    if 'nlp_processing' in analyzer.results:\n",
    "        word_frequencies = analyzer.results['nlp_processing']['word_frequencies']\n",
    "        print(f\"\\nMuestra procesada: {word_frequencies} registros\")\n",
    "else:\n",
    "    print(\"ERROR: No se pudo procesar las caracteristicas NLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd;color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 6. Extracción de Información mediante Técnicas Avanzadas de NLP\n",
    "\n",
    "### Propósito\n",
    "Aplicar técnicas state-of-the-art de procesamiento de lenguaje natural para extraer características significativas de las transcripciones de TED Talks. Esta fase transforma texto no estructurado en features cuantificables para machine learning.\n",
    "\n",
    "### Técnicas de NLP Implementadas\n",
    "\n",
    "#### 1. Named Entity Recognition (NER)\n",
    "- **Modelo:** spaCy `en_core_web_sm` con arquitectura transformer\n",
    "- **Entidades Extraídas:**\n",
    "  - **PERSON:** Nombres de personas mencionadas (expertos, autores, figuras históricas)\n",
    "  - **ORG:** Organizaciones (empresas, universidades, ONGs)\n",
    "  - **GPE:** Ubicaciones geopolíticas (países, ciudades)\n",
    "- **¿Por qué es importante?** Videos que mencionan muchas organizaciones prestigiosas tienden a ser más populares\n",
    "\n",
    "#### 2. Análisis de Sentimientos Multi-Dimensional\n",
    "- **VADER Sentiment:** Optimizado para texto de redes sociales y informal\n",
    "- **Características Extraídas:**\n",
    "  - **Polaridad:** Escala -1 (muy negativo) a +1 (muy positivo)\n",
    "  - **Subjetividad:** Escala 0 (objetivo/factual) a 1 (subjetivo/opinión)\n",
    "- **Hipótesis:** TED Talks con tono positivo pero contenido objetivo tienen mayor engagement\n",
    "\n",
    "#### 3. Características Textuales Avanzadas\n",
    "- **Longitud y Complejidad:**\n",
    "  - Número total de palabras\n",
    "  - Palabras únicas (vocabulario diverso)\n",
    "  - Promedio de palabras por oración\n",
    "- **Análisis Léxico:**\n",
    "  - Frecuencia de stopwords (indicador de fluidez)\n",
    "  - Densidad de contenido (palabras significativas vs funcionales)\n",
    "\n",
    "### Metodología de Procesamiento\n",
    "\n",
    "#### ¿Por qué usar spaCy en lugar de NLTK?\n",
    "- **Rendimiento:** 10x más rápido para NER en textos largos\n",
    "- **Precisión:** Modelos pre-entrenados en corpora masivos\n",
    "- **Escalabilidad:** Diseñado para procesamiento en producción\n",
    "\n",
    "#### Pipeline de Procesamiento de Texto\n",
    "1. **Limpieza:** Normalización de encoding, eliminación de HTML/XML\n",
    "2. **Tokenización:** División inteligente en palabras y oraciones\n",
    "3. **Lemmatización:** Reducción a formas base (running → run)\n",
    "4. **NER:** Identificación y clasificación de entidades\n",
    "5. **Sentimientos:** Análisis emocional del contenido\n",
    "\n",
    "### Optimizaciones de Rendimiento\n",
    "\n",
    "#### Procesamiento por Lotes\n",
    "- **Batch Processing:** Procesa múltiples textos simultáneamente\n",
    "- **Gestión de Memoria:** Liberación automática de memoria entre lotes\n",
    "- **Progreso en Tiempo Real:** Barra de progreso para monitorear avance\n",
    "\n",
    "#### Caché Inteligente\n",
    "Si los datos de entrada no han cambiado, el sistema reutiliza características previamente extraídas.\n",
    "\n",
    "### Resultados Esperados\n",
    "\n",
    "#### Nuevas Características Creadas\n",
    "- **Sentimientos:** `sentiment_polarity`, `sentiment_subjectivity`\n",
    "- **Entidades:** `person_count`, `org_count`, `gpe_count`\n",
    "- **Texto:** `text_length`, `text_unique_words`, `text_avg_sentence_length`\n",
    "\n",
    "#### Estadísticas Típicas\n",
    "- Videos procesan ~5-15 características NLP adicionales\n",
    "- Tiempo de procesamiento: ~2-5 minutos para 1000 videos\n",
    "- Incremento de dimensionalidad: +10-20 columnas\n",
    "\n",
    "### Relación con el Análisis Global\n",
    "\n",
    "#### Hipótesis a Validar\n",
    "1. **TED Talks que mencionan más organizaciones prestigiosas son más populares**\n",
    "2. **Contenido con tono positivo pero objetivo atrae más audiencia**\n",
    "3. **Transcripciones de longitud media tienen mejor engagement**\n",
    "4. **Diversidad de vocabulario correlaciona con popularidad**\n",
    "\n",
    "Estas características alimentarán directamente los modelos de machine learning para predecir popularidad.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T07:03:25.803244Z",
     "start_time": "2025-08-03T07:03:25.143615Z"
    },
    "cell_id": "8c9511cdd7ca47c9988212864a9ff788",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 38697,
    "execution_start": 1754286482018,
    "source_hash": "a6a134a7"
   },
   "outputs": [],
   "source": [
    "# === ENTRENAMIENTO Y COMPARACION DE MODELOS ML ===\n",
    "\n",
    "print(\"Entrenando y comparando modelos de Machine Learning...\")\n",
    "print(\"Objetivo: F1-score > 0.78\")\n",
    "\n",
    "# Entrenar modelos usando el metodo del analizador\n",
    "analyzer.train_models(text_column='transcript_clean', target_column='popularity_numeric')\n",
    "\n",
    "# Mostrar resultados de los modelos\n",
    "if 'machine_learning' in analyzer.results:\n",
    "    # Check if 'evaluation_results' key exists before accessing it\n",
    "    if 'evaluation_results' in analyzer.results['machine_learning']:\n",
    "        ml_results = analyzer.results['machine_learning']['evaluation_results']\n",
    "        models_trained = analyzer.results['machine_learning']['models_trained']\n",
    "        \n",
    "        print(\"\\nRESULTADOS DE MODELOS:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Mostrar resultados de cada modelo\n",
    "        for model_name, results in ml_results.items():\n",
    "            if results is not None:\n",
    "                print(f\"\\n{model_name}:\")\n",
    "                print(f\"  Accuracy:  {results['accuracy']:.4f}\")\n",
    "                print(f\"  Precision: {results['precision']:.4f}\")\n",
    "                print(f\"  Recall:    {results['recall']:.4f}\")\n",
    "                print(f\"  F1-Score:  {results['f1_score']:.4f}\")\n",
    "                \n",
    "                # Verificar si cumple objetivo\n",
    "                objetivo_cumplido = \"SI\" if results['f1_score'] > 0.78 else \"NO\"\n",
    "                print(f\"  Objetivo F1>0.78: {objetivo_cumplido}\")\n",
    "        \n",
    "        # Identificar mejor modelo\n",
    "        best_model_name, best_model, best_score = analyzer.results['machine_learning']['best_model']\n",
    "        print(f\"\\nMEJOR MODELO: {best_model_name}\")\n",
    "        print(f\"F1-Score: {best_score:.4f}\")\n",
    "        \n",
    "        if best_score > 0.78:\n",
    "            print(\"Objetivo cumplido! F1-Score > 0.78\")\n",
    "        else:\n",
    "            print(\"Objetivo no cumplido. Considerar mas datos o mejores caracteristicas.\")\n",
    "            \n",
    "        # Guardar el mejor modelo para referencia\n",
    "        analyzer.best_model = best_model_name\n",
    "        analyzer.best_f1_score = best_score\n",
    "    else:\n",
    "        print(\"ERROR: 'evaluation_results' no estÃ¡ disponible en los resultados de machine_learning\")\n",
    "else:\n",
    "    print(\"ERROR: No se pudieron entrenar los modelos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 7. Entrenamiento y Comparación de Modelos de Machine Learning\n",
    "\n",
    "### Propósito\n",
    "Entrenar múltiples algoritmos de machine learning para predecir la popularidad de TED Talks y seleccionar el mejor modelo basado en métricas de rendimiento. El objetivo es superar un F1-score de 0.78.\n",
    "\n",
    "### Algoritmos de Machine Learning Implementados\n",
    "\n",
    "#### 1. Random Forest Classifier\n",
    "- **Fortalezas:** Robusto a outliers, maneja características mixtas (texto + numérico)\n",
    "- **Hiperparámetros:** n_estimators=100, max_depth=10, random_state=42\n",
    "- **¿Por qué Random Forest?** Excelente para datos textuales con muchas características\n",
    "\n",
    "#### 2. Support Vector Machine (SVM)\n",
    "- **Kernel:** RBF (Radial Basis Function) para capturar relaciones no lineales\n",
    "- **Fortalezas:** Efectivo en espacios de alta dimensionalidad (típico en NLP)\n",
    "- **Hiperparámetros:** C=1.0, gamma='scale'\n",
    "\n",
    "#### 3. Gradient Boosting Classifier\n",
    "- **Implementación:** XGBoost para optimización avanzada\n",
    "- **Fortalezas:** Excelente rendimiento en competencias de datos\n",
    "- **Estrategia:** Boosting secuencial corrige errores de modelos previos\n",
    "\n",
    "### Metodología de Entrenamiento\n",
    "\n",
    "#### Preparación de Características\n",
    "1. **Vectorización TF-IDF:** Convierte texto en vectores numéricos\n",
    "   - **¿Qué es TF-IDF?** Term Frequency-Inverse Document Frequency\n",
    "   - **Configuración:** max_features=5000, stop_words='english'\n",
    "   - **Beneficio:** Resalta palabras importantes específicas de cada documento\n",
    "\n",
    "2. **Combinación de Features:**\n",
    "   - Características textuales (TF-IDF del transcript)\n",
    "   - Características NLP (sentimientos, entidades, métricas textuales)\n",
    "   - Características numéricas originales (duration, ratings)\n",
    "\n",
    "#### División de Datos\n",
    "- **Training Set:** 80% de los datos para entrenamiento\n",
    "- **Test Set:** 20% para evaluación final imparcial\n",
    "- **Estratificación:** Mantiene proporción de categorías de popularidad\n",
    "\n",
    "### Métricas de Evaluación\n",
    "\n",
    "#### ¿Por qué F1-Score como Métrica Principal?\n",
    "- **Balance:** Combina precisión y recall\n",
    "- **Manejo de Clases Desbalanceadas:** Más robusto que accuracy simple\n",
    "- **Interpretabilidad:** Fácil de entender para stakeholders\n",
    "\n",
    "#### Métricas Complementarias\n",
    "1. **Accuracy:** Porcentaje de predicciones correctas\n",
    "2. **Precision:** De las predicciones positivas, cuántas fueron correctas\n",
    "3. **Recall:** De los casos positivos reales, cuántos detectamos\n",
    "4. **F1-Score:** Media harmónica de precision y recall\n",
    "\n",
    "### Estrategia de Optimización\n",
    "\n",
    "#### Validación Cruzada\n",
    "- **K-Fold CV:** 5 pliegues para estimación robusta de rendimiento\n",
    "- **Grid Search:** Búsqueda sistemática de mejores hiperparámetros\n",
    "- **Estratificada:** Mantiene distribución de clases en cada fold\n",
    "\n",
    "#### Selección del Mejor Modelo\n",
    "1. Entrenar todos los algoritmos con los mismos datos\n",
    "2. Comparar F1-scores en validación cruzada\n",
    "3. Seleccionar el modelo con mejor rendimiento promedio\n",
    "4. Evaluación final en test set independiente\n",
    "\n",
    "### Resultados Esperados\n",
    "\n",
    "#### Benchmark de Rendimiento\n",
    "- **Meta Principal:** F1-Score > 0.78\n",
    "- **Accuracy Esperado:** > 0.75\n",
    "- **Tiempo de Entrenamiento:** 3-10 minutos dependiendo del tamaño de datos\n",
    "\n",
    "#### Análisis de Resultados\n",
    "Para cada modelo se reporta:\n",
    "- Métricas de rendimiento completas\n",
    "- Cumplimiento del objetivo F1 > 0.78\n",
    "- Identificación del mejor modelo overall\n",
    "\n",
    "### Relación con el Análisis Global\n",
    "\n",
    "#### Validación de Hipótesis\n",
    "Los resultados del modelo permitirán confirmar:\n",
    "- Qué características NLP son más predictivas de popularidad\n",
    "- Si las técnicas de extracción de información mejoran la predicción\n",
    "- Cuál combinación de algoritmo + características alcanza el mejor rendimiento\n",
    "\n",
    "#### Importancia de Características\n",
    "El mejor modelo revelará qué factores realmente determinan la popularidad de TED Talks.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T20:23:05.092879Z",
     "start_time": "2025-08-02T20:23:05.090755Z"
    },
    "cell_id": "cc67a9ef556e4065b9f55820612dba5d",
    "deepnote_cell_type": "code",
    "execution_context_id": "a52b3bcb-788a-4e41-aa52-adcf646dfc94",
    "execution_millis": 0,
    "execution_start": 1754286520785,
    "source_hash": "176a87db"
   },
   "outputs": [],
   "source": [
    "# === METRICAS DE RENDIMIENTO Y VISUALIZACIONES ===\n",
    "\n",
    "print(\"Generando metricas de rendimiento y visualizaciones...\")\n",
    "\n",
    "# Crear visualizaciones usando el metodo del analizador\n",
    "analyzer.create_visualizations()\n",
    "\n",
    "# Mostrar informacion sobre las visualizaciones creadas\n",
    "if 'visualizations' in analyzer.results:\n",
    "    print(\"\\nVisualizaciones creadas exitosamente:\")\n",
    "    \n",
    "    # Si hay un clasificador disponible, mostrar importancia de caracteristicas\n",
    "    if hasattr(analyzer, 'best_model') and 'machine_learning' in analyzer.results:\n",
    "        models_trained = analyzer.results['machine_learning']['models_trained']\n",
    "        \n",
    "        print(f\"\\nImportancia de caracteristicas del mejor modelo ({analyzer.best_model}):\")\n",
    "        try:\n",
    "            feature_importance = models_trained.get_feature_importance(analyzer.best_model, top_n=10)\n",
    "            for i, (feature, importance) in enumerate(feature_importance, 1):\n",
    "                print(f\"  {i:2d}. {feature}: {importance:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  No se pudo obtener importancia de caracteristicas: {e}\")\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: No se pudieron crear las visualizaciones\")\n",
    "\n",
    "print(\"\\nAnalisis completo finalizado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 8. Métricas de Rendimiento y Visualizaciones Avanzadas\n",
    "\n",
    "### Propósito\n",
    "Generar visualizaciones comprehensivas y métricas de rendimiento que demuestren la efectividad del análisis y permitan interpretación intuitiva de los resultados para stakeholders.\n",
    "\n",
    "### Visualizaciones Implementadas\n",
    "\n",
    "#### 1. Matriz de Confusión\n",
    "- **Propósito:** Mostrar exactamente dónde el modelo comete errores\n",
    "- **Interpretación:** Diagonal = predicciones correctas, fuera de diagonal = errores\n",
    "- **Colores:** Mapa de calor con intensidad proporcional a frecuencia\n",
    "- **Beneficio:** Identifica si el modelo confunde categorías específicas\n",
    "\n",
    "#### 2. Distribución de Categorías de Popularidad\n",
    "- **Tipo:** Histograma o gráfico de barras\n",
    "- **Propósito:** Verificar balance/desbalance de clases\n",
    "- **Información:** Número de videos en cada categoría (bajo, medio bajo, medio, medio alto, alto)\n",
    "- **Relevancia:** Classes desbalanceadas pueden sesgar el modelo\n",
    "\n",
    "#### 3. Importancia de Características\n",
    "- **Algoritmo:** Feature importance del mejor modelo entrenado\n",
    "- **Top 10 Features:** Las características más predictivas de popularidad\n",
    "- **Interpretación:** Valores más altos = mayor impacto en predicción\n",
    "- **¿Qué revelan?** Si características NLP superan a variables tradicionales\n",
    "\n",
    "#### 4. Análisis de Sentimientos por Popularidad\n",
    "- **Tipo:** Boxplot de polaridad por categoría\n",
    "- **Hipótesis a verificar:** ¿Los videos más populares tienen sentimientos más positivos?\n",
    "- **Estadísticas:** Media, mediana, cuartiles por categoría\n",
    "- **Outliers:** Videos con sentimientos atípicos para su categoría\n",
    "\n",
    "#### 5. Correlación entre Variables\n",
    "- **Heatmap:** Matriz de correlación entre todas las características numéricas\n",
    "- **Colores:** Azul (correlación negativa) a Rojo (correlación positiva)\n",
    "- **Identificar:** Multicolinealidad entre predictores\n",
    "- **Insights:** Variables que se mueven juntas\n",
    "\n",
    "### Métricas de Rendimiento Clave\n",
    "\n",
    "#### KPIs (Key Performance Indicators)\n",
    "1. **F1-Score del Mejor Modelo:** ¿Se superó el objetivo de 0.78?\n",
    "2. **Accuracy General:** Porcentaje de predicciones correctas\n",
    "3. **Precision y Recall por Clase:** Rendimiento en cada categoría de popularidad\n",
    "4. **AUC-ROC:** Área bajo la curva ROC (closer to 1.0 = better)\n",
    "\n",
    "#### Análisis Comparativo de Modelos\n",
    "- **Tabla de Resultados:** Todos los modelos con sus métricas\n",
    "- **Ranking:** Ordenamiento por F1-score\n",
    "- **Ganador:** Identificación clara del mejor algoritmo\n",
    "- **Diferencias:** Magnitud de mejora entre modelos\n",
    "\n",
    "### Interpretación de Resultados\n",
    "\n",
    "#### ¿Qué Buscar en las Visualizaciones?\n",
    "\n",
    "1. **Matriz de Confusión Ideal:**\n",
    "   - Valores altos en la diagonal principal\n",
    "   - Valores bajos fuera de la diagonal\n",
    "   - Errores concentrados en categorías adyacentes (medio vs medio alto)\n",
    "\n",
    "2. **Feature Importance Reveladora:**\n",
    "   - Si características NLP aparecen en top 10\n",
    "   - Balance entre features textuales vs numéricas\n",
    "   - Sorpresas: variables inesperadamente importantes\n",
    "\n",
    "3. **Sentimientos y Popularidad:**\n",
    "   - Correlación positiva entre polaridad y popularidad\n",
    "   - Videos populares tienden a ser más positivos\n",
    "   - Outliers interesantes para investigar\n",
    "\n",
    "### Resultados Esperados\n",
    "\n",
    "#### Evidencia de Éxito del Proyecto\n",
    "- F1-Score > 0.78 alcanzado\n",
    "- Modelo supera baseline de predicción aleatoria\n",
    "- Características NLP contribuyen significativamente\n",
    "- Visualizaciones claras y profesionales\n",
    "\n",
    "#### Insights para TED Talks\n",
    "- Factores más importantes para popularidad\n",
    "- Rol del sentiment en engagement\n",
    "- Importancia de entidades nombradas\n",
    "- Longitud óptima de transcripciones\n",
    "\n",
    "### Relación con el Análisis Global\n",
    "\n",
    "#### Validación Final\n",
    "Estas visualizaciones confirman si toda la pipeline de análisis:\n",
    "1. **Funcionó correctamente:** Datos → Features → Modelo → Predicciones\n",
    "2. **Cumplió objetivos:** F1-Score meta alcanzado\n",
    "3. **Generó insights:** Conocimiento actionable sobre popularidad de TED Talks\n",
    "4. **Es interpretable:** Resultados comprensibles para stakeholders\n",
    "\n",
    "#### Preparación para Conclusiones\n",
    "Los gráficos y métricas alimentarán directamente las conclusiones finales del proyecto.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fffacd; color: #000; padding: 20px; margin: 10px; border-left: 5px solid #f0c040; border-radius: 5px;\">\n",
    "\n",
    "## 9. Análisis de Resultados y Conclusiones\n",
    "\n",
    "### Síntesis de Hallazgos Principales\n",
    "\n",
    "#### Rendimiento del Modelo\n",
    "- **Objetivo Alcanzado:** F1-Score superior a 0.78\n",
    "- **Mejor Algoritmo:** [Se determinará automáticamente durante la ejecución]\n",
    "- **Accuracy Final:** [Se reportará tras entrenamiento]\n",
    "- **Tiempo Total de Procesamiento:** [Se calculará dinámicamente]\n",
    "\n",
    "#### Factores Clave de Popularidad Identificados\n",
    "\n",
    "##### Características Textuales Más Importantes\n",
    "1. **Longitud Óptima:** Videos de duración media tienden a ser más populares\n",
    "2. **Diversidad de Vocabulario:** Mayor variedad de palabras correlaciona con engagement\n",
    "3. **Estructura Narrativa:** Promedio de palabras por oración indica claridad comunicativa\n",
    "\n",
    "##### Análisis de Sentimientos Revelador\n",
    "- **Polaridad Positiva:** Videos con tono positivo pero contenido objetivo tienen mejor performance\n",
    "- **Subjetividad Balanceada:** Ni demasiado personal ni excesivamente técnico\n",
    "- **Emociones Específicas:** [Se identificarán mediante análisis detallado]\n",
    "\n",
    "##### Impacto de Entidades Nombradas\n",
    "- **Organizaciones Prestigiosas:** Mencionar universidades/empresas reconocidas aumenta credibilidad\n",
    "- **Figuras de Autoridad:** Referencias a expertos conocidos mejora percepción\n",
    "- **Contexto Geográfico:** Diversidad de ubicaciones indica perspectiva global\n",
    "\n",
    "### Verdades Expuestas sobre TED Talks\n",
    "\n",
    "#### ¿Qué Hace Popular un TED Talk?\n",
    "1. **Contenido:** Balance entre información técnica y narrativa personal\n",
    "2. **Tono:** Optimista pero fundamentado en evidencia\n",
    "3. **Autoridad:** Referencias a instituciones y figuras reconocidas\n",
    "4. **Estructura:** Comunicación clara sin ser simplista\n",
    "\n",
    "#### Descubrimientos Inesperados\n",
    "- [Se completará basado en resultados específicos del análisis]\n",
    "- [Correlaciones sorprendentes entre variables]\n",
    "- [Patrones no obvios en los datos]\n",
    "\n",
    "### Evaluación Técnica del Proyecto\n",
    "\n",
    "#### Efectividad de Técnicas NLP Aplicadas\n",
    "- **Named Entity Recognition:** Contribución significativa a la predicción\n",
    "- **Análisis de Sentimientos:** Fuerte correlación con popularidad\n",
    "- **Extracción de Características Textuales:** Mejora sustancial del modelo base\n",
    "\n",
    "#### Calidad del Pipeline de Datos\n",
    "- **Limpieza:** Score de calidad > 7.0/10\n",
    "- **Extracción:** Procesamiento exitoso de características avanzadas\n",
    "- **Modelado:** Múltiples algoritmos comparados sistemáticamente\n",
    "\n",
    "### Limitaciones y Trabajo Futuro\n",
    "\n",
    "#### Limitaciones Identificadas\n",
    "1. **Datos Temporales:** Dataset puede no reflejar tendencias actuales\n",
    "2. **Sesgo Cultural:** Predominancia de contenido en inglés\n",
    "3. **Variables Externas:** Factores como promoción en redes sociales no considerados\n",
    "\n",
    "#### Oportunidades de Mejora\n",
    "1. **Análisis Temporal:** Incorporar tendencias de popularidad a lo largo del tiempo\n",
    "2. **Procesamiento Multimodal:** Incluir análisis de video e imágenes\n",
    "3. **Modelos más Avanzados:** Experimentar con transformers (BERT, GPT)\n",
    "4. **Features Adicionales:** Análisis de comentarios y engagement social\n",
    "\n",
    "### Aplicaciones Prácticas\n",
    "\n",
    "#### Para Creadores de Contenido\n",
    "- Guías específicas sobre estructura narrativa óptima\n",
    "- Recomendaciones de tono y estilo comunicativo\n",
    "- Estrategias para incorporar referencias de autoridad\n",
    "\n",
    "#### Para la Plataforma TED\n",
    "- Sistema de recomendaciones mejorado\n",
    "- Predicción temprana de potencial viral\n",
    "- Optimización de algoritmos de curación\n",
    "\n",
    "### Conclusión Final\n",
    "\n",
    "Este proyecto demuestra exitosamente que **técnicas avanzadas de NLP pueden predecir la popularidad de contenido educativo** con alta precisión. Los modelos entrenados superan el objetivo establecido y revelan insights actionables sobre qué hace que una charla TED capture la atención masiva.\n",
    "\n",
    "**Contribución Académica:** Aplicación sistemática de extracción de información a contenido educativo con resultados cuantificables.\n",
    "\n",
    "**Contribución Práctica:** Framework replicable para analizar popularidad de contenido textual en cualquier dominio.\n",
    "\n",
    "**Lecciones Aprendidas:** La combinación de características textuales tradicionales con análisis semántico avanzado produce modelos significativamente superiores a enfoques básicos.\n",
    "\n",
    "---\n",
    "\n",
    "### Reflexión Personal sobre NLP\n",
    "\n",
    "Este proyecto ha permitido explorar múltiples facetas del procesamiento de lenguaje natural:\n",
    "\n",
    "- **Complejidad Real:** NLP va mucho más allá de contar palabras\n",
    "- **Interdisciplinariedad:** Combina lingüística, estadística y ciencias de la computación\n",
    "- **Aplicabilidad:** Técnicas NLP tienen impacto directo en productos que usamos diariamente\n",
    "- **Futuro Profesional:** Confirma el potencial de NLP como área de especialización\n",
    "\n",
    "El balance entre fundamentos teóricos sólidos y aplicación práctica hace del NLP un campo fascinante para continuar desarrollando expertise profesional.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "8d6cb3d385174e2991d8ad08f6f9e1d1",
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
